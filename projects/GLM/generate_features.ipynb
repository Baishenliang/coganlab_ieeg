{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Generate features (acoustic, phonetic, and lexical) for stimuli\n",
    "## 0. Some basic functions\n",
    "### 0.1. Test multicollinearity"
   ],
   "id": "eb93b739e88b6f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "lme_dir = 'D:\\\\bsliang_Coganlabcode\\\\coganlab_ieeg\\\\projects\\\\lme\\\\data'\n",
    "\n",
    "def check_multicollinearity(X_i,corr_labels):\n",
    "    \n",
    "    # Ensure the input is a NumPy array\n",
    "    if not isinstance(X_i, np.ndarray):\n",
    "        raise ValueError(\"Input X_i must be a NumPy array\")\n",
    "\n",
    "    # Compute VIF for each feature\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = corr_labels\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X_i, i) for i in range(X_i.shape[1])]\n",
    "\n",
    "    # Plot correlation matrix as a heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    correlation_matrix = np.corrcoef(X_i, rowvar=False)  # Compute correlation matrix\n",
    "    sns.heatmap(correlation_matrix, annot=True, fmt=\".1f\", cmap=\"coolwarm\", xticklabels=corr_labels, yticklabels=corr_labels,annot_kws={\"size\": 5})\n",
    "    plt.title(\"Feature Correlation Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    return vif_data"
   ],
   "id": "f4c37ade548672f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 0.2. PCA\n",
   "id": "76a0a760b81ea2e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def perform_pca_on_acoustic_features(envelope_feature_dict, original_feature_names,n_components=None, visualize=True, min_variance_retention=0.95):\n",
    "    \"\"\"\n",
    "    Performs Principal Component Analysis (PCA) on a dictionary of acoustic features.\n",
    "\n",
    "    Args:\n",
    "        envelope_feature_dict (dict): A dictionary where keys are sample/word identifiers\n",
    "                                      (e.g., 'bacon', 'bagel') and values are lists\n",
    "                                      of their corresponding acoustic features (e.g., power envelope means\n",
    "                                      across different frequency bands).\n",
    "        n_components (int or float or None, optional): The number of components to keep after PCA.\n",
    "            - If int: The exact number of principal components to return.\n",
    "            - If float (0.0 < n_components < 1.0): The proportion of variance to explain.\n",
    "              PCA will select the minimum number of components such that the explained variance\n",
    "              is greater than or equal to this value.\n",
    "            - If None (default): All components will be kept. The function will then\n",
    "              suggest an optimal number based on `min_variance_retention`.\n",
    "        visualize (bool, optional): If True, generate plots for explained variance and\n",
    "                                     (if applicable) a scatter plot of the first two principal components.\n",
    "                                     Defaults to True.\n",
    "        min_variance_retention (float, optional): Only used if `n_components` is None.\n",
    "            Specifies the minimum cumulative explained variance ratio (between 0 and 1)\n",
    "            to retain for suggesting the number of components. Defaults to 0.95.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - pca_transformed_df (pd.DataFrame): DataFrame with the transformed principal components.\n",
    "                                                Rows are samples, columns are principal components (PC1, PC2, ...).\n",
    "            - pca_model (sklearn.decomposition.PCA): The fitted PCA model object.\n",
    "            - scaler_model (sklearn.preprocessing.StandardScaler): The fitted StandardScaler model object.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"--- Starting PCA Analysis ---\")\n",
    "\n",
    "    # 1. Convert the dictionary to a Pandas DataFrame\n",
    "    # Each row will represent a sample (word), and each column will represent a feature (frequency band).\n",
    "    df = pd.DataFrame.from_dict(envelope_feature_dict, orient='index')\n",
    "\n",
    "    print(f\"\\nOriginal data shape: {df.shape} (samples x features)\")\n",
    "    # print(\"Original data (first 5 rows):\\n\", df.head()) # Uncomment for full data view\n",
    "\n",
    "    # 2. Data Standardization\n",
    "    # PCA is sensitive to feature scales, so standardization is a crucial step.\n",
    "    # StandardScaler transforms data to have a mean of 0 and a standard deviation of 1.\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(df)\n",
    "    scaled_df = pd.DataFrame(scaled_data, index=df.index, columns=df.columns)\n",
    "    # print(\"\\nScaled data (first 5 rows):\\n\", scaled_df.head()) # Uncomment for full data view\n",
    "\n",
    "    # 3. Perform PCA (initial fit to get explained variance, if n_components is not directly specified)\n",
    "    # If n_components is already specified, we'll directly use it.\n",
    "    if n_components is None:\n",
    "        pca = PCA() # Fit without specifying components to analyze explained variance\n",
    "        pca.fit(scaled_data)\n",
    "        explained_variance_ratio = pca.explained_variance_ratio_\n",
    "        cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "        print(\"\\nExplained variance per principal component:\\n\", explained_variance_ratio)\n",
    "        print(\"\\nCumulative explained variance:\\n\", cumulative_explained_variance)\n",
    "\n",
    "        # Suggest optimal number of components based on min_variance_retention\n",
    "        if len(cumulative_explained_variance) > 0:\n",
    "            suggested_n_components = np.where(cumulative_explained_variance >= min_variance_retention)[0][0] + 1\n",
    "            print(f\"\\nTo retain at least {min_variance_retention*100:.0f}% of the variance, \"\n",
    "                  f\"it's suggested to select {suggested_n_components} principal components.\")\n",
    "            final_n_components = suggested_n_components\n",
    "        else:\n",
    "            final_n_components = 0\n",
    "            print(\"No principal components could be determined from the data.\")\n",
    "    else:\n",
    "        # If n_components is provided, use it directly\n",
    "        final_n_components = n_components\n",
    "        print(f\"\\nProceeding with PCA using n_components = {final_n_components}\")\n",
    "\n",
    "    # 4. Re-execute PCA with the determined or specified number of components\n",
    "    if final_n_components > 0:\n",
    "        pca_model = PCA(n_components=final_n_components)\n",
    "        principal_components = pca_model.fit_transform(scaled_data)\n",
    "\n",
    "        # Convert principal components to a DataFrame for easier handling\n",
    "        pc_column_names = [f'PC{i+1}' for i in range(principal_components.shape[1])]\n",
    "        pca_transformed_df = pd.DataFrame(data=principal_components, index=df.index, columns=pc_column_names)\n",
    "        print(f\"\\nFinal PCA transformed data shape: {pca_transformed_df.shape}\")\n",
    "        print(\"PCA transformed data (first 5 rows):\\n\", pca_transformed_df.head())\n",
    "    else:\n",
    "        pca_transformed_df = pd.DataFrame(index=df.index) # Empty DataFrame if no components\n",
    "        pca_model = None\n",
    "        print(\"PCA could not be performed with the specified parameters or data.\")\n",
    "        \n",
    "    # 5. Normolize results\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    # Apply Min-Max Scaling to the PCA transformed DataFrame\n",
    "    normalized_pca_data = min_max_scaler.fit_transform(pca_transformed_df)\n",
    "    normalized_pca_df = pd.DataFrame(normalized_pca_data,\n",
    "                                     index=pca_transformed_df.index,\n",
    "                                     columns=pca_transformed_df.columns)\n",
    "    \n",
    "    # 6. get loadings\n",
    "    loadings_df = pd.DataFrame(pca_model.components_,\n",
    "                               columns=original_feature_names,\n",
    "                               index=pc_column_names)\n",
    "    \n",
    "    # 8. Visualization (Optional)\n",
    "    if visualize and pca_model is not None:\n",
    "        # Plot Explained Variance Ratio (if full PCA was done initially)\n",
    "        if n_components is None and len(cumulative_explained_variance) > 0:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(range(1, len(explained_variance_ratio) + 1), cumulative_explained_variance, marker='o', linestyle='--')\n",
    "            plt.axhline(y=min_variance_retention, color='r', linestyle=':', label=f'{min_variance_retention*100:.0f}% Variance')\n",
    "            plt.axvline(x=final_n_components, color='g', linestyle=':', label=f'{final_n_components} Components')\n",
    "            plt.title('PCA - Cumulative Explained Variance')\n",
    "            plt.xlabel('Number of Principal Components')\n",
    "            plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "            plt.xticks(range(1, len(explained_variance_ratio) + 1))\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        # Scatter plot of the first two principal components (if at least 2 components are kept)\n",
    "        # if final_n_components >= 2:\n",
    "        #     plt.figure(figsize=(8, 6))\n",
    "        #     sns.scatterplot(x=pca_transformed_df.iloc[:, 0], y=pca_transformed_df.iloc[:, 1],\n",
    "        #                     hue=pca_transformed_df.index, s=100, palette='viridis')\n",
    "        #     plt.title('PCA - Scatter Plot of First Two Principal Components')\n",
    "        #     plt.xlabel(f'Principal Component 1 (explains {pca_model.explained_variance_ratio_[0]*100:.2f}%)')\n",
    "        #     plt.ylabel(f'Principal Component 2 (explains {pca_model.explained_variance_ratio_[1]*100:.2f}%)')\n",
    "        #     plt.grid(True)\n",
    "        #     # plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        #     plt.tight_layout()\n",
    "        #     plt.show()\n",
    "        # elif final_n_components == 1:\n",
    "        #     print(\"\\nOnly one principal component was retained. A 2D scatter plot is not applicable.\")\n",
    "        # --- New: Plotting Feature Loadings (Contributions) ---\n",
    "        \n",
    "        print(\"\\n--- Visualizing Feature Loadings ---\")\n",
    "        # pca_model.components_ gives the principal axes in feature space (loadings)\n",
    "        # It has shape (n_components, n_features)\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(loadings_df, cmap='coolwarm', annot=True, fmt=\".2f\", linewidths=.5, linecolor='black')\n",
    "        plt.title('Principal Component Loadings (Contribution of Original Features to PCs)')\n",
    "        plt.xlabel('Original Features')\n",
    "        plt.ylabel('Principal Components')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Optional: Bar plot for individual PC loadings (e.g., PC1)\n",
    "        if final_n_components >= 1:\n",
    "            for pc in ['PC1', 'PC2', 'PC3', 'PC4', 'PC5','PC6']:\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                loadings_df.loc[pc].plot(kind='bar')\n",
    "                plt.title(f'Loadings of Original Features on Principal Component 1 {pc}')\n",
    "                plt.xlabel('Original Features')\n",
    "                plt.ylabel('Loading Value')\n",
    "                plt.xticks(rotation=45, ha='right')\n",
    "                plt.grid(axis='y', linestyle='--')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            \n",
    "        print(\"\\n--- Visualizing Clustered Feature Correlations ---\")\n",
    "        # Calculate the correlation matrix of the original (scaled) features\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        # Use sns.clustermap which performs hierarchical clustering by default\n",
    "        # The 'cbar_kws' argument helps to control the color bar label\n",
    "        sns.clustermap(loadings_df, cmap='coolwarm', annot=True, fmt=\".2f\",\n",
    "                       linewidths=.5, figsize=(12, 10),\n",
    "                       cbar_kws={'label': 'Correlation Coefficient'})\n",
    "        plt.suptitle('Clustered Heatmap of Original Feature Correlations', y=1.02) # Adjusted title position\n",
    "        plt.show()\n",
    "            \n",
    "    print(\"\\n--- PCA Analysis Complete ---\")\n",
    "    return normalized_pca_df, pca_model, loadings_df"
   ],
   "id": "bfc3e156c7bdd1d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Phonemic feature matrix\n",
    "### 1.1. Get MFA dictionary"
   ],
   "id": "cbd0fd88acb08b7a"
  },
  {
   "cell_type": "code",
   "id": "be958ee1025a0abe",
   "metadata": {},
   "source": [
    "def read_syllable_dict(file_path):\n",
    "    \"\"\"\n",
    "    Reads a file and returns a dictionary with words as keys and phoneme lists as values.\n",
    "    Ignores the numeric middle section between tabs, extracting only word and phonemes.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the file\n",
    "    \n",
    "    Returns:\n",
    "        dict: {word: [phonemes]}\n",
    "    \"\"\"\n",
    "    syllable_dict = {}\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Remove leading/trailing whitespace and split by tabs\n",
    "            parts = line.strip().split('\\t')\n",
    "\n",
    "            # Extract word (first part) and phonemes (third part)\n",
    "            word = parts[0].replace(\"'\", \"\")  # Remove single quotes if present\n",
    "            if len(parts) == 6:\n",
    "                phonemes_str = parts[5]  # Third part is the phonemes\n",
    "            elif len(parts) == 2:\n",
    "                phonemes_str = parts[1]  # Third part is the phonemes\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            # Split phonemes by spaces into a list\n",
    "            phonemes = phonemes_str.split()\n",
    "            if not phonemes:  # If no phonemes found, skip the line\n",
    "                print(f\"No phonemes found in line: {line.strip()}\")\n",
    "                continue\n",
    "                \n",
    "            # Store in dictionary\n",
    "            syllable_dict[word] = phonemes\n",
    "    \n",
    "    return syllable_dict"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "file_path = \"english_us_lr.dict\"\n",
    "# Create the filename-to-phoneme dictionary\n",
    "dict = read_syllable_dict(file_path)\n",
    "for word, syllables in list(dict.items())[:10]:\n",
    "    print(f\"{word}: {syllables}\")"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.2. Get the stimuli phonemic dictionary for the Lexical Delay Experiment",
   "id": "c4eec62ff796ecbc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "def get_wav_files(directory):\n",
    "    wav_files_names = {}\n",
    "    wav_files = []\n",
    "    # Check if directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory '{directory}' does not exist.\")\n",
    "        return wav_files\n",
    "    \n",
    "    # Iterate through files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.lower().endswith('.wav'):  # Case-insensitive check for .wav files\n",
    "            # Add to dictionary (key is filename, value can be None or customized)\n",
    "            wav_files_names[filename[:-4]] = None\n",
    "    return wav_files_names"
   ],
   "id": "24e802fb196587e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_phoneme_start_time_dict(syllables, base_path:str='C:\\\\Users\\\\bl314\\\\Box\\\\CoganLab\\\\ECoG_Task_Data\\\\Stim\\\\Lexical\\\\mfa\\\\stim_annotations'):\n",
    "    phoneme_start_times = {}\n",
    "    for word in syllables.keys():\n",
    "        print(f'getting timestamps of word {word}')\n",
    "        file_path = os.path.join(base_path, f\"{word}_phones.txt\")\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                start_times = []\n",
    "                for i, line in enumerate(f):\n",
    "                    if i < 5: \n",
    "                        parts = line.strip().split('\\t')\n",
    "                        if parts:\n",
    "                            start_times.append(float(parts[0]))\n",
    "                    else:\n",
    "                        break\n",
    "                phoneme_start_times[word] = start_times\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Not found {file_path}. Skipped {word}\")\n",
    "    return phoneme_start_times"
   ],
   "id": "bf972c38dc76df97",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "directory = r\"C:\\Users\\bl314\\Box\\CoganLab\\task_stimuli\\LexicalDecRepDelay\"\n",
    "syllables = get_wav_files(directory)\n",
    "for syllable in syllables.keys():\n",
    "    syllables[syllable] = dict.get(syllable)\n",
    "    if syllable=='pilot' and len(syllables[syllable])<5:\n",
    "        syllables[syllable].append('T')\n",
    "    print(f\"{syllable}: {syllables[syllable]}\")\n",
    "syllables_timestamps = create_phoneme_start_time_dict(syllables)\n",
    "syllables_timestamps_DF=pd.DataFrame(syllables_timestamps)\n",
    "print(syllables_timestamps_DF)\n",
    "# Get time of the first phoneme\n",
    "print(np.mean(syllables_timestamps_DF.iloc[1]))\n",
    "print(np.std(syllables_timestamps_DF.iloc[1]))\n",
    "\n",
    "print(np.mean(syllables_timestamps_DF.iloc[2]))\n",
    "print(np.std(syllables_timestamps_DF.iloc[2]))\n"
   ],
   "id": "3312bae6530128c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#For LME: save phoneme categories and timestamps for each syllable\n",
    "import pickle\n",
    "filename = os.path.join(lme_dir,'stim_pho_dict.pkl')\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(syllables, f)\n",
    "filename = os.path.join(lme_dir,'stim_pho_t_dict.pkl')\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(syllables_timestamps, f)\n"
   ],
   "id": "6fbcf128a324f67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.3. One-hot encoding for each unique phoneme",
   "id": "ebc1600d8644bbeb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get unique phoneme vector\n",
    "all_phonemes = set()\n",
    "for phonemes in syllables.values():\n",
    "    all_phonemes.update(phonemes)\n",
    "all_phonemes = sorted(list(all_phonemes))  # Sort for consistent ordering\n",
    "phoneme_to_index = {phoneme: idx for idx, phoneme in enumerate(all_phonemes)}\n",
    "vector_length = len(all_phonemes)\n",
    "\n",
    "# Create one-hot encoding for each word\n",
    "phoneme_one_hot_dict = {}\n",
    "for word, phonemes in syllables.items():\n",
    "    # Initialize vector with zeros\n",
    "    vector = [0] * vector_length\n",
    "    # Set 1 for each phoneme present in the word\n",
    "    for phoneme in phonemes:\n",
    "        vector[phoneme_to_index[phoneme]] = 1\n",
    "    phoneme_one_hot_dict[word] = vector"
   ],
   "id": "4ecf8a359cd2f974",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check the multicolinearity and reduce the feature dimensions\n",
    "check_multicollinearity(np.array(list(phoneme_one_hot_dict.values())),all_phonemes)\n",
    "# Print the words tha contain AE0, W, ER0, and Y\n",
    "for syllable, value in syllables.items():\n",
    "    if value and \"AE0\" in value:\n",
    "        print(f\"{syllable}: {value} contains AE0\")\n",
    "    if value and \"W\" in value:\n",
    "        print(f\"{syllable}: {value} contains W\")\n",
    "    if value and \"ER0\" in value:\n",
    "        print(f\"{syllable}: {value} contains ER0\")\n",
    "    if value and \"Y\" in value:\n",
    "        print(f\"{syllable}: {value} contains Y\")\n",
    "    if value and \"UW1\" in value:\n",
    "        print(f\"{syllable}: {value} contains UW1\")\n",
    "    if value and \"HH\" in value and \"M\" in value:\n",
    "        print(f\"{syllable}: {value} contains both HH and M\")\n",
    "    if value and \"T\" in value and \"L\" in value and \"AH0\" in value:\n",
    "        print(f\"{syllable}: {value} contains T, L and AH0\")\n",
    "    if value and \"OW1\" in value and \"M\" in value and \"N\" in value:\n",
    "        print(f\"{syllable}: {value} contains OW1, M and N\")\n",
    "# I think it is safe to reduce ER0, Y, UW1, AE0, and W\n",
    "# Then: humor - HH and M, tulip - 'T', 'L', 'AH0', 'P', women - 'OW1', 'M', 'N'\n",
    "\n",
    "# reduce the dimensions:\n",
    "indices_to_remove=[all_phonemes.index(\"ER0\"),all_phonemes.index(\"Y\"),all_phonemes.index(\"UW1\"),all_phonemes.index(\"AE0\"),all_phonemes.index(\"W\")]\n",
    "phoneme_one_hot_dict_filt = {key: vec for key, vec in phoneme_one_hot_dict.items()}\n",
    "for i, (key, vec) in enumerate(phoneme_one_hot_dict.items()):\n",
    "    # Create a copy of the vector with specified indices removed\n",
    "    filtered_vec = np.delete(vec, indices_to_remove)\n",
    "    phoneme_one_hot_dict_filt[key] = filtered_vec\n",
    "all_phonemes_filt = []\n",
    "all_phonemes_filt = [p for i, p in enumerate(all_phonemes) if i not in indices_to_remove]\n",
    "\n",
    "# Check the multicolinearity again\n",
    "check_multicollinearity(np.array(list(phoneme_one_hot_dict_filt.values())),all_phonemes_filt)"
   ],
   "id": "fac5189faebf939f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# PCA\n",
    "print(\"\\n--- Running PCA with automatic component selection (95% variance) ---\")\n",
    "pca_results_auto, _, _ = perform_pca_on_acoustic_features(\n",
    "    phoneme_one_hot_dict_filt,original_feature_names=all_phonemes_filt,\n",
    "    n_components=None, # Let the function decide\n",
    "    min_variance_retention=0.9\n",
    ")\n",
    "phoneme_one_hot_dict_filt_pca = pca_results_auto.to_dict('index')\n",
    "phoneme_one_hot_dict_filt_pca = {key: list(value.values()) for key, value in phoneme_one_hot_dict_filt_pca.items()}\n",
    "\n",
    "for key, values in list(phoneme_one_hot_dict_filt_pca.items())[:5]:  # Show first 5 entries\n",
    "    print(key, \":\", values)\n"
   ],
   "id": "cac23726dd7ff726",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# save the phoneme_one_hot_dicphoneme_one_hot_dictt\n",
    "import pickle\n",
    "phoneme_one_hot_dict_path = \"phoneme_one_hot_dict.pickle\"\n",
    "with open(phoneme_one_hot_dict_path, 'wb') as handle:\n",
    "    pickle.dump(phoneme_one_hot_dict_filt, handle)\n",
    "phoneme_one_hot_dict_filt"
   ],
   "id": "4056e594099a3dda",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "phoneme_one_hot_dict_filt_df = pd.DataFrame(phoneme_one_hot_dict_filt)\n",
    "phoneme_one_hot_dict_filt_df.to_csv(os.path.join(lme_dir,'phoneme_one_hot_dict.csv'), index=False)\n",
    "phoneme_one_hot_dict_filt_pca_df = pd.DataFrame(phoneme_one_hot_dict_filt_pca)\n",
    "phoneme_one_hot_dict_filt_pca_df.to_csv(os.path.join(lme_dir,'phoneme_one_hot_dict_pca.csv'), index=False)"
   ],
   "id": "5c58caf0c19a5ca0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Read the saved one hot dict. Just for demonstration\n",
    "# import pandas as pd\n",
    "# d= pd.read_pickle(\"phoneme_one_hot_dict.pickle\")"
   ],
   "id": "4d40eb0a2ff5ab49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Acoustic feature (Power envelope at five frequency bins)\n",
    "### 2.1. Read normolized binned envelope \n",
    "(The matrix was generated by `get_stims_envelope.m`)"
   ],
   "id": "34373c32f714de93"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "envelope_feature_dict = {}\n",
    "\n",
    "with open(\"envelope_power_bins.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        key = parts[0]\n",
    "        values = list(map(float, parts[1:]))  # Convert remaining columns to float\n",
    "        envelope_feature_dict[key] = values\n",
    "feature_names=['50.00 Hz','120.58 Hz','209.04 Hz','319.88 Hz','458.78 Hz','632.84 Hz','850.96 Hz','1124.30 Hz','1466.84 Hz','1896.08 Hz','2433.98 Hz','3108.04 Hz','3952.74 Hz','5011.26 Hz','6337.74 Hz','8000.00 Hz']\n",
    "# Print a sample of the dictionary\n",
    "for key, values in list(envelope_feature_dict.items())[:5]:  # Show first 5 entries\n",
    "    print(key, \":\", values)\n",
    "import pandas as pd"
   ],
   "id": "14cdba23da27aeaa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "envelope_feature_dict_df = pd.DataFrame(envelope_feature_dict)\n",
    "envelope_feature_dict_df.to_csv(os.path.join(lme_dir,'envelope_feature_dict.csv'), index=False)"
   ],
   "id": "aa7f2013a642d765",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Check the multicolinearity and reduce the feature dimensions\n",
    "check_multicollinearity(np.array(list(envelope_feature_dict.values())),\n",
    "                        feature_names)"
   ],
   "id": "d57a46752f7686b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# PCA\n",
    "print(\"\\n--- Running PCA with automatic component selection (95% variance) ---\")\n",
    "pca_results_auto, _, _ = perform_pca_on_acoustic_features(\n",
    "    envelope_feature_dict,original_feature_names=feature_names,\n",
    "    n_components=None, # Let the function decide\n",
    "    min_variance_retention=0.9\n",
    ")\n",
    "envelope_feature_dict_pca = pca_results_auto.to_dict('index')\n",
    "envelope_feature_dict_pca = {key: list(value.values()) for key, value in envelope_feature_dict_pca.items()}\n",
    "\n",
    "for key, values in list(envelope_feature_dict_pca.items())[:5]:  # Show first 5 entries\n",
    "    print(key, \":\", values)"
   ],
   "id": "153453573d37ec14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "check_multicollinearity(np.array(list(envelope_feature_dict_pca.values())),\n",
    "                        ['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9'])"
   ],
   "id": "fabf8032a3dcc125",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save dictionary\n",
    "import pickle\n",
    "envelope_feature_dict_path = \"envelope_feature_dict.pickle\"\n",
    "with open(envelope_feature_dict_path, 'wb') as handle:\n",
    "    pickle.dump(envelope_feature_dict_pca, handle)"
   ],
   "id": "a2926683ad6d0bad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "envelope_feature_dict_pca = pd.DataFrame(envelope_feature_dict_pca)\n",
    "envelope_feature_dict_pca.to_csv(os.path.join(lme_dir,'envelope_feature_dict_pca.csv'), index=False)"
   ],
   "id": "d1ed364b36bc5d81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Acoustic+Phonemeic feature (for a whole token)\n",
   "id": "63130fbc5b907a69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def classify_pca(loadings_df, ratio_threshold=6/4):\n",
    "    hz_cols = [col for col in loadings_df.columns if 'Hz' in col]\n",
    "    other_cols = [col for col in loadings_df.columns if 'Hz' not in col]\n",
    "    \n",
    "    if len(hz_cols) == 0 or len(other_cols) == 0:\n",
    "        raise ValueError(\"Loadings DataFrame must contain at least one 'Hz' feature column and one non-'Hz' feature column.\")\n",
    "\n",
    "    pc_new_names = {}\n",
    "    \n",
    "    aco_counter = 1\n",
    "    pho_counter = 1\n",
    "    acpho_counter = 1\n",
    "\n",
    "    for pc_idx, row in loadings_df.iterrows():\n",
    "        aco_w = np.mean(np.abs(row[hz_cols]))\n",
    "        pho_w = np.mean(np.abs(row[other_cols]))\n",
    "\n",
    "        if pho_w / aco_w > ratio_threshold:\n",
    "            new_name = f'pho{pho_counter}'\n",
    "            pho_counter += 1\n",
    "        elif aco_w / pho_w > ratio_threshold:\n",
    "            new_name = f'aco{aco_counter}'\n",
    "            aco_counter += 1\n",
    "        else:\n",
    "            new_name = f'mix{acpho_counter}'\n",
    "            acpho_counter += 1\n",
    "            \n",
    "        pc_new_names[pc_idx] = new_name\n",
    "    \n",
    "    return pc_new_names"
   ],
   "id": "df037e41f1f8e69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load dictionary of HG responses and make a lexical status mapping\n",
    "from ieeg.arrays.label import LabeledArray\n",
    "script_dir = 'D:\\\\bsliang_Coganlabcode\\\\coganlab_ieeg\\\\projects\\\\lme\\\\data'\n",
    "epoc_LexDelayRep_Aud = LabeledArray.fromfile(os.path.join(script_dir,'epoc_LexDelayRep_Aud'))\n",
    "stim_lexsts = epoc_LexDelayRep_Aud.labels[0].split('/')[:,[3,2]]\n",
    "del epoc_LexDelayRep_Aud\n",
    "lexsts_dict = {str(item[0]): str(item[1]) for item in stim_lexsts}\n",
    "lexsts_bi_dict = {key: [1] if value == 'Word' else [0] for key, value in lexsts_dict.items()}\n",
    "lexsts_bi_dict['magic']=[1]\n",
    "print(lexsts_bi_dict)"
   ],
   "id": "b988f4a37ca3784",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "aco_pho_lex_dict = {key: lexsts_bi_dict[key] + envelope_feature_dict[key] + phoneme_one_hot_dict_filt[key].tolist()  for key in envelope_feature_dict}\n",
    "check_multicollinearity(np.array(list(aco_pho_lex_dict.values())),\n",
    "                        ['lex']+feature_names+all_phonemes_filt)"
   ],
   "id": "57cb9eb9eeb6d5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pca_results_aco_pho, _, loadings_df = perform_pca_on_acoustic_features(\n",
    "    aco_pho_lex_dict,original_feature_names=['lex']+feature_names+all_phonemes_filt,\n",
    "    n_components=None, # Let the function decide\n",
    "    min_variance_retention=0.9\n",
    ")\n",
    "aco_pho_dict_pca = pca_results_aco_pho.to_dict('index')\n",
    "aco_pho_dict_dict_pca = {key: list(value.values()) for key, value in aco_pho_dict_pca.items()}"
   ],
   "id": "d73dc20a976f5205",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "aco_pho_dict_pca_df = pd.DataFrame(aco_pho_dict_pca)\n",
    "print(aco_pho_dict_pca_df)\n",
    "pc_new_names=classify_pca(loadings_df,ratio_threshold=6/4)\n",
    "aco_pho_dict_pca_df.rename(index=pc_new_names, inplace=True)\n",
    "print(aco_pho_dict_pca_df)\n",
    "sns.heatmap(loadings_df.rename(index=pc_new_names))\n",
    "print(loadings_df.T)\n",
    "aco_pho_dict_pca_df.to_csv(os.path.join(lme_dir,'aco_pho_dict_pca.csv'), index=True)"
   ],
   "id": "f5eef80aa41bb3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Acoustic+Phonemeic feature (pho1 or 2 only)\n",
   "id": "bb94cecb8e872edb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pho=2",
   "id": "a43ec701ce1d77b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get acoustic features\n",
    "phoX_aco_dict = {}\n",
    "with open(f\"envelope_power_bins_pho{pho}.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        key = parts[0]\n",
    "        values = list(map(float, parts[1:]))  # Convert remaining columns to float\n",
    "        phoX_aco_dict[key] = values\n",
    "feature_names=['50.00 Hz','120.58 Hz','209.04 Hz','319.88 Hz','458.78 Hz','632.84 Hz','850.96 Hz','1124.30 Hz','1466.84 Hz','1896.08 Hz','2433.98 Hz','3108.04 Hz','3952.74 Hz','5011.26 Hz','6337.74 Hz','8000.00 Hz']\n",
    "# Print a sample of the dictionary\n",
    "for key, values in list(phoX_aco_dict.items())[:5]:  # Show first 5 entries\n",
    "    print(key, \":\", values)\n",
    "check_multicollinearity(np.array(list(phoX_aco_dict.values())),\n",
    "                        feature_names)"
   ],
   "id": "6da24775a2e60100",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get phoneme vector for phoX\n",
    "all_phonemes_phoX = set()\n",
    "for phonemes in syllables.values():\n",
    "    all_phonemes_phoX.add(phonemes[pho-1])\n",
    "print(all_phonemes_phoX)\n",
    "all_phonemes_phoX = sorted(list(all_phonemes_phoX))  # Sort for consistent ordering\n",
    "phoneme_to_index = {phoneme: idx for idx, phoneme in enumerate(all_phonemes_phoX)}\n",
    "vector_length = len(all_phonemes_phoX)\n",
    "\n",
    "# Create one-hot encoding for each word (only pho1)\n",
    "phoX_pho_dict = {}\n",
    "for word, phonemes in syllables.items():\n",
    "    # Initialize vector with zeros\n",
    "    vector = [0] * vector_length\n",
    "    # Set 1 for each phoneme present in the word\n",
    "    vector[phoneme_to_index[phonemes[pho-1]]] = 1\n",
    "    phoX_pho_dict[word] = vector\n",
    "\n",
    "check_multicollinearity(np.array(list(phoX_pho_dict.values())),\n",
    "                        all_phonemes_phoX)"
   ],
   "id": "b43569217ff900f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Combine the two and run PCA\n",
    "phoX_aco_pho_dict = {key: phoX_aco_dict[key] + phoX_pho_dict[key]\n",
    "                    for key in phoX_aco_dict}\n",
    "check_multicollinearity(np.array(list(phoX_aco_pho_dict.values())),\n",
    "                        feature_names+all_phonemes_phoX)\n",
    "phoX_aco_pho_dict_pca_df, _, loadings_df = perform_pca_on_acoustic_features(\n",
    "    phoX_aco_pho_dict,original_feature_names=feature_names+all_phonemes_phoX,\n",
    "    n_components=None, # Let the function decide\n",
    "    min_variance_retention=0.9\n",
    ")\n",
    "print(loadings_df)\n",
    "phoX_aco_pho_dict_pca_df=phoX_aco_pho_dict_pca_df.T\n",
    "print(phoX_aco_pho_dict_pca_df)\n",
    "pc_new_names=classify_pca(loadings_df,ratio_threshold=6/4)\n",
    "phoX_aco_pho_dict_pca_df.rename(index=pc_new_names, inplace=True)\n",
    "print(phoX_aco_pho_dict_pca_df)\n",
    "loadings_df=np.abs(loadings_df)\n",
    "sns.heatmap(loadings_df.rename(index=pc_new_names))\n",
    "phoX_aco_pho_dict_pca_df.to_csv(os.path.join(lme_dir,f'pho{pho}_aco_pho_dict_pca.csv'), index=True)\n"
   ],
   "id": "ae1b7b382d480729",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Acoustic+Phonemeic feature (syllable1 only)\n",
   "id": "ae0c5de5ed097ae6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get acoustic features\n",
    "syl1_aco_dict = {}\n",
    "with open(\"envelope_power_bins_syl1.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        key = parts[0]\n",
    "        values = list(map(float, parts[1:]))  # Convert remaining columns to float\n",
    "        syl1_aco_dict[key] = values\n",
    "feature_names=['50.00 Hz','120.58 Hz','209.04 Hz','319.88 Hz','458.78 Hz','632.84 Hz','850.96 Hz','1124.30 Hz','1466.84 Hz','1896.08 Hz','2433.98 Hz','3108.04 Hz','3952.74 Hz','5011.26 Hz','6337.74 Hz','8000.00 Hz']\n",
    "# Print a sample of the dictionary\n",
    "for key, values in list(syl1_aco_dict.items())[:5]:  # Show first 5 entries\n",
    "    print(key, \":\", values)\n",
    "check_multicollinearity(np.array(list(syl1_aco_dict.values())),\n",
    "                        feature_names)"
   ],
   "id": "e932d24762316394",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get phoneme vector for pho1\n",
    "all_phonemes_syl1 = set()\n",
    "for phonemes in syllables.values():\n",
    "    all_phonemes_syl1.add(phonemes[0])\n",
    "    all_phonemes_syl1.add(phonemes[1])\n",
    "all_phonemes_syl1 = sorted(list(all_phonemes_syl1))  # Sort for consistent ordering\n",
    "phoneme_to_index = {phoneme: idx for idx, phoneme in enumerate(all_phonemes_syl1)}\n",
    "vector_length = len(all_phonemes_syl1)\n",
    "\n",
    "# Create one-hot encoding for each word (only pho1)\n",
    "syl1_pho_dict = {}\n",
    "for word, phonemes in syllables.items():\n",
    "    # Initialize vector with zeros\n",
    "    vector = [0] * vector_length\n",
    "    # Set 1 for each phoneme present in the word\n",
    "    vector[phoneme_to_index[phonemes[0]]] = 1\n",
    "    vector[phoneme_to_index[phonemes[1]]] = 1\n",
    "    syl1_pho_dict[word] = vector\n",
    "\n",
    "check_multicollinearity(np.array(list(syl1_pho_dict.values())),\n",
    "                        all_phonemes_syl1)"
   ],
   "id": "ae50f243704a131",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Combine the two and run PCA\n",
    "syl1_aco_pho_dict = {key: syl1_aco_dict[key] + syl1_pho_dict[key]\n",
    "                    for key in syl1_aco_dict}\n",
    "check_multicollinearity(np.array(list(syl1_aco_pho_dict.values())),\n",
    "                        feature_names+all_phonemes_syl1)\n",
    "syl1_aco_pho_dict_pca_df, _, loadings_df = perform_pca_on_acoustic_features(\n",
    "    syl1_aco_pho_dict,original_feature_names=feature_names+all_phonemes_syl1,\n",
    "    n_components=None, # Let the function decide\n",
    "    min_variance_retention=0.9\n",
    ")\n",
    "print(loadings_df)\n",
    "syl1_aco_pho_dict_pca_df=syl1_aco_pho_dict_pca_df.T\n",
    "print(syl1_aco_pho_dict_pca_df)\n",
    "pc_new_names=classify_pca(loadings_df,ratio_threshold=6/4)\n",
    "syl1_aco_pho_dict_pca_df.rename(index=pc_new_names, inplace=True)\n",
    "print(syl1_aco_pho_dict_pca_df)\n",
    "loadings_df=np.abs(loadings_df)\n",
    "sns.heatmap(loadings_df.rename(index=pc_new_names))\n",
    "syl1_aco_pho_dict_pca_df.to_csv(os.path.join(lme_dir,'syl1_aco_pho_dict_pca.csv'), index=True)"
   ],
   "id": "a766d74794da4220",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
