{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb93b739e88b6f7",
   "metadata": {},
   "source": [
    "# Generate features (acoustic, phonetic, and lexical) for stimuli\n",
    "## 0. Some basic functions\n",
    "### 0.1. Test multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c37ade548672f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T03:44:33.399174Z",
     "start_time": "2025-10-27T03:44:33.336893Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\", \"..\")))\n",
    "import utils.group as gp\n",
    "lme_dir = 'D:\\\\bsliang_Coganlabcode\\\\coganlab_ieeg\\\\projects\\\\lme\\\\data'\n",
    "\n",
    "def check_multicollinearity(X_i,corr_labels):\n",
    "    \n",
    "    # Ensure the input is a NumPy array\n",
    "    if not isinstance(X_i, np.ndarray):\n",
    "        raise ValueError(\"Input X_i must be a NumPy array\")\n",
    "\n",
    "    # Compute VIF for each feature\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = corr_labels\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X_i, i) for i in range(X_i.shape[1])]\n",
    "\n",
    "    # Plot correlation matrix as a heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    correlation_matrix = np.corrcoef(X_i, rowvar=False)  # Compute correlation matrix\n",
    "    sns.heatmap(correlation_matrix, annot=True, fmt=\".1f\", cmap=\"coolwarm\", xticklabels=corr_labels, yticklabels=corr_labels,annot_kws={\"size\": 5})\n",
    "    plt.title(\"Feature Correlation Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    return vif_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a0a760b81ea2e1",
   "metadata": {},
   "source": [
    "### 0.2. PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc3e156c7bdd1d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T03:44:40.015430Z",
     "start_time": "2025-10-27T03:44:39.967373Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def perform_pca_on_acoustic_features(envelope_feature_dict, original_feature_names,n_components=None, visualize=True, min_variance_retention=0.95):\n",
    "    \"\"\"\n",
    "    Performs Principal Component Analysis (PCA) on a dictionary of acoustic features.\n",
    "\n",
    "    Args:\n",
    "        envelope_feature_dict (dict): A dictionary where keys are sample/word identifiers\n",
    "                                      (e.g., 'bacon', 'bagel') and values are lists\n",
    "                                      of their corresponding acoustic features (e.g., power envelope means\n",
    "                                      across different frequency bands).\n",
    "        n_components (int or float or None, optional): The number of components to keep after PCA.\n",
    "            - If int: The exact number of principal components to return.\n",
    "            - If float (0.0 < n_components < 1.0): The proportion of variance to explain.\n",
    "              PCA will select the minimum number of components such that the explained variance\n",
    "              is greater than or equal to this value.\n",
    "            - If None (default): All components will be kept. The function will then\n",
    "              suggest an optimal number based on `min_variance_retention`.\n",
    "        visualize (bool, optional): If True, generate plots for explained variance and\n",
    "                                     (if applicable) a scatter plot of the first two principal components.\n",
    "                                     Defaults to True.\n",
    "        min_variance_retention (float, optional): Only used if `n_components` is None.\n",
    "            Specifies the minimum cumulative explained variance ratio (between 0 and 1)\n",
    "            to retain for suggesting the number of components. Defaults to 0.95.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - pca_transformed_df (pd.DataFrame): DataFrame with the transformed principal components.\n",
    "                                                Rows are samples, columns are principal components (PC1, PC2, ...).\n",
    "            - pca_model (sklearn.decomposition.PCA): The fitted PCA model object.\n",
    "            - scaler_model (sklearn.preprocessing.StandardScaler): The fitted StandardScaler model object.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"--- Starting PCA Analysis ---\")\n",
    "\n",
    "    # 1. Convert the dictionary to a Pandas DataFrame\n",
    "    # Each row will represent a sample (word), and each column will represent a feature (frequency band).\n",
    "    df = pd.DataFrame.from_dict(envelope_feature_dict, orient='index')\n",
    "\n",
    "    print(f\"\\nOriginal data shape: {df.shape} (samples x features)\")\n",
    "    # print(\"Original data (first 5 rows):\\n\", df.head()) # Uncomment for full data view\n",
    "\n",
    "    # 2. Data Standardization\n",
    "    # PCA is sensitive to feature scales, so standardization is a crucial step.\n",
    "    # StandardScaler transforms data to have a mean of 0 and a standard deviation of 1.\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(df)\n",
    "    scaled_df = pd.DataFrame(scaled_data, index=df.index, columns=df.columns)\n",
    "    # print(\"\\nScaled data (first 5 rows):\\n\", scaled_df.head()) # Uncomment for full data view\n",
    "\n",
    "    # 3. Perform PCA (initial fit to get explained variance, if n_components is not directly specified)\n",
    "    # If n_components is already specified, we'll directly use it.\n",
    "    if n_components is None:\n",
    "        pca = PCA() # Fit without specifying components to analyze explained variance\n",
    "        pca.fit(scaled_data)\n",
    "        explained_variance_ratio = pca.explained_variance_ratio_\n",
    "        cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "        print(\"\\nExplained variance per principal component:\\n\", explained_variance_ratio)\n",
    "        print(\"\\nCumulative explained variance:\\n\", cumulative_explained_variance)\n",
    "\n",
    "        # Suggest optimal number of components based on min_variance_retention\n",
    "        if len(cumulative_explained_variance) > 0:\n",
    "            suggested_n_components = np.where(cumulative_explained_variance >= min_variance_retention)[0][0] + 1\n",
    "            print(f\"\\nTo retain at least {min_variance_retention*100:.0f}% of the variance, \"\n",
    "                  f\"it's suggested to select {suggested_n_components} principal components.\")\n",
    "            final_n_components = suggested_n_components\n",
    "        else:\n",
    "            final_n_components = 0\n",
    "            print(\"No principal components could be determined from the data.\")\n",
    "    else:\n",
    "        # If n_components is provided, use it directly\n",
    "        final_n_components = n_components\n",
    "        print(f\"\\nProceeding with PCA using n_components = {final_n_components}\")\n",
    "\n",
    "    # 4. Re-execute PCA with the determined or specified number of components\n",
    "    if final_n_components > 0:\n",
    "        pca_model = PCA(n_components=final_n_components)\n",
    "        principal_components = pca_model.fit_transform(scaled_data)\n",
    "\n",
    "        # Convert principal components to a DataFrame for easier handling\n",
    "        pc_column_names = [f'PC{i+1}' for i in range(principal_components.shape[1])]\n",
    "        pca_transformed_df = pd.DataFrame(data=principal_components, index=df.index, columns=pc_column_names)\n",
    "        print(f\"\\nFinal PCA transformed data shape: {pca_transformed_df.shape}\")\n",
    "        print(\"PCA transformed data (first 5 rows):\\n\", pca_transformed_df.head())\n",
    "    else:\n",
    "        pca_transformed_df = pd.DataFrame(index=df.index) # Empty DataFrame if no components\n",
    "        pca_model = None\n",
    "        print(\"PCA could not be performed with the specified parameters or data.\")\n",
    "        \n",
    "    # 5. Normolize results\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    # Apply Min-Max Scaling to the PCA transformed DataFrame\n",
    "    normalized_pca_data = min_max_scaler.fit_transform(pca_transformed_df)\n",
    "    normalized_pca_df = pd.DataFrame(normalized_pca_data,\n",
    "                                     index=pca_transformed_df.index,\n",
    "                                     columns=pca_transformed_df.columns)\n",
    "    \n",
    "    # 6. get loadings\n",
    "    loadings_df = pd.DataFrame(pca_model.components_,\n",
    "                               columns=original_feature_names,\n",
    "                               index=pc_column_names)\n",
    "    \n",
    "    # 8. Visualization (Optional)\n",
    "    if visualize and pca_model is not None:\n",
    "        # Plot Explained Variance Ratio (if full PCA was done initially)\n",
    "        if n_components is None and len(cumulative_explained_variance) > 0:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(range(1, len(explained_variance_ratio) + 1), cumulative_explained_variance, marker='o', linestyle='--')\n",
    "            plt.axhline(y=min_variance_retention, color='r', linestyle=':', label=f'{min_variance_retention*100:.0f}% Variance')\n",
    "            plt.axvline(x=final_n_components, color='g', linestyle=':', label=f'{final_n_components} Components')\n",
    "            plt.title('PCA - Cumulative Explained Variance')\n",
    "            plt.xlabel('Number of Principal Components')\n",
    "            plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "            plt.xticks(range(1, len(explained_variance_ratio) + 1))\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        # Scatter plot of the first two principal components (if at least 2 components are kept)\n",
    "        # if final_n_components >= 2:\n",
    "        #     plt.figure(figsize=(8, 6))\n",
    "        #     sns.scatterplot(x=pca_transformed_df.iloc[:, 0], y=pca_transformed_df.iloc[:, 1],\n",
    "        #                     hue=pca_transformed_df.index, s=100, palette='viridis')\n",
    "        #     plt.title('PCA - Scatter Plot of First Two Principal Components')\n",
    "        #     plt.xlabel(f'Principal Component 1 (explains {pca_model.explained_variance_ratio_[0]*100:.2f}%)')\n",
    "        #     plt.ylabel(f'Principal Component 2 (explains {pca_model.explained_variance_ratio_[1]*100:.2f}%)')\n",
    "        #     plt.grid(True)\n",
    "        #     # plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        #     plt.tight_layout()\n",
    "        #     plt.show()\n",
    "        # elif final_n_components == 1:\n",
    "        #     print(\"\\nOnly one principal component was retained. A 2D scatter plot is not applicable.\")\n",
    "        # --- New: Plotting Feature Loadings (Contributions) ---\n",
    "        \n",
    "        print(\"\\n--- Visualizing Feature Loadings ---\")\n",
    "        # pca_model.components_ gives the principal axes in feature space (loadings)\n",
    "        # It has shape (n_components, n_features)\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(loadings_df, cmap='coolwarm', annot=True, fmt=\".2f\", linewidths=.5, linecolor='black')\n",
    "        plt.title('Principal Component Loadings (Contribution of Original Features to PCs)')\n",
    "        plt.xlabel('Original Features')\n",
    "        plt.ylabel('Principal Components')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Optional: Bar plot for individual PC loadings (e.g., PC1)\n",
    "        if final_n_components >= 1:\n",
    "            for pc in ['PC1', 'PC2', 'PC3', 'PC4', 'PC5']:\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                loadings_df.loc[pc].plot(kind='bar')\n",
    "                plt.title(f'Loadings of Original Features on Principal Component 1 {pc}')\n",
    "                plt.xlabel('Original Features')\n",
    "                plt.ylabel('Loading Value')\n",
    "                plt.xticks(rotation=45, ha='right')\n",
    "                plt.grid(axis='y', linestyle='--')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            \n",
    "        print(\"\\n--- Visualizing Clustered Feature Correlations ---\")\n",
    "        # Calculate the correlation matrix of the original (scaled) features\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        # Use sns.clustermap which performs hierarchical clustering by default\n",
    "        # The 'cbar_kws' argument helps to control the color bar label\n",
    "        sns.clustermap(loadings_df, cmap='coolwarm', annot=True, fmt=\".2f\",\n",
    "                       linewidths=.5, figsize=(12, 10),\n",
    "                       cbar_kws={'label': 'Correlation Coefficient'})\n",
    "        plt.suptitle('Clustered Heatmap of Original Feature Correlations', y=1.02) # Adjusted title position\n",
    "        plt.show()\n",
    "            \n",
    "    print(\"\\n--- PCA Analysis Complete ---\")\n",
    "    return normalized_pca_df, pca_model, loadings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06e552147dfb3d8",
   "metadata": {},
   "source": [
    "###Get lexical status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e0f112fa9012a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T03:47:33.479545Z",
     "start_time": "2025-10-27T03:44:50.321914Z"
    }
   },
   "outputs": [],
   "source": [
    "HOME = os.path.expanduser(\"~\")\n",
    "LAB_root = os.path.join(HOME, \"Box\", \"CoganLab\")\n",
    "\n",
    "stats_root_delay = os.path.join(LAB_root, 'BIDS-1.0_LexicalDecRepDelay', 'BIDS', \"derivatives\", \"stats\")\n",
    "stats_root_nodelay = os.path.join(LAB_root, 'BIDS-1.0_LexicalDecRepNoDelay', 'BIDS', \"derivatives\", \"stats\")\n",
    "\n",
    "epoc_LexDelayRep_Aud,_=gp.load_stats('zscore','Auditory_inRep','epo',stats_root_delay,stats_root_delay,keeptrials=True,testsubj=True)\n",
    "\n",
    "stim_lexsts = epoc_LexDelayRep_Aud.labels[0].split('/')[:,[3,2]]\n",
    "del epoc_LexDelayRep_Aud\n",
    "lexsts_dict = {str(item[0].split('-')[0]): str(item[1]) for item in stim_lexsts}\n",
    "lexsts_bi_dict = {key: [1] if value == 'Word' else [0] for key, value in lexsts_dict.items()}\n",
    "lexsts_bi_dict['magic']=[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd0fd88acb08b7a",
   "metadata": {},
   "source": [
    "## 1. Phonemic feature matrix\n",
    "### 1.1. Get MFA dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be958ee1025a0abe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T03:51:55.035134Z",
     "start_time": "2025-10-27T03:51:55.030594Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_syllable_dict(file_path):\n",
    "    \"\"\"\n",
    "    Reads a file and returns a dictionary with words as keys and phoneme lists as values.\n",
    "    Ignores the numeric middle section between tabs, extracting only word and phonemes.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the file\n",
    "    \n",
    "    Returns:\n",
    "        dict: {word: [phonemes]}\n",
    "    \"\"\"\n",
    "    syllable_dict = {}\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Remove leading/trailing whitespace and split by tabs\n",
    "            parts = line.strip().split('\\t')\n",
    "\n",
    "            # Extract word (first part) and phonemes (third part)\n",
    "            word = parts[0].replace(\"'\", \"\")  # Remove single quotes if present\n",
    "            if len(parts) == 6:\n",
    "                phonemes_str = parts[5]  # Third part is the phonemes\n",
    "            elif len(parts) == 2:\n",
    "                phonemes_str = parts[1]  # Third part is the phonemes\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            # Split phonemes by spaces into a list\n",
    "            phonemes = phonemes_str.split()\n",
    "            if not phonemes:  # If no phonemes found, skip the line\n",
    "                print(f\"No phonemes found in line: {line.strip()}\")\n",
    "                continue\n",
    "                \n",
    "            # Store in dictionary\n",
    "            syllable_dict[word] = phonemes\n",
    "    \n",
    "    return syllable_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T03:51:59.441591Z",
     "start_time": "2025-10-27T03:51:59.192697Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path = \"english_us_lr.dict\"\n",
    "# Create the filename-to-phoneme dictionary\n",
    "dict = read_syllable_dict(file_path)\n",
    "for word, syllables in list(dict.items())[:10]:\n",
    "    print(f\"{word}: {syllables}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eec62ff796ecbc",
   "metadata": {},
   "source": [
    "### 1.2. Get the stimuli phonemic dictionary for the Lexical Delay Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e802fb196587e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T03:52:04.784605Z",
     "start_time": "2025-10-27T03:52:04.780202Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_wav_files(directory):\n",
    "    wav_files_names = {}\n",
    "    wav_files = []\n",
    "    # Check if directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory '{directory}' does not exist.\")\n",
    "        return wav_files\n",
    "    \n",
    "    # Iterate through files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.lower().endswith('.wav'):  # Case-insensitive check for .wav files\n",
    "            # Add to dictionary (key is filename, value can be None or customized)\n",
    "            wav_files_names[filename[:-4]] = None\n",
    "    return wav_files_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf972c38dc76df97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T03:52:07.356531Z",
     "start_time": "2025-10-27T03:52:07.351578Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_phoneme_start_time_dict(syllables, base_path:str='C:\\\\Users\\\\bl314\\\\Box\\\\CoganLab\\\\ECoG_Task_Data\\\\Stim\\\\Lexical\\\\mfa\\\\stim_annotations'):\n",
    "    phoneme_start_times = {}\n",
    "    for word in syllables.keys():\n",
    "        print(f'getting timestamps of word {word}')\n",
    "        file_path = os.path.join(base_path, f\"{word}_phones.txt\")\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                start_times = []\n",
    "                for i, line in enumerate(f):\n",
    "                    if i < 5: \n",
    "                        parts = line.strip().split('\\t')\n",
    "                        if parts:\n",
    "                            start_times.append(float(parts[0]))\n",
    "                    else:\n",
    "                        break\n",
    "                phoneme_start_times[word] = start_times\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Not found {file_path}. Skipped {word}\")\n",
    "    return phoneme_start_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3312bae6530128c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T03:54:40.880179Z",
     "start_time": "2025-10-27T03:52:12.571652Z"
    }
   },
   "outputs": [],
   "source": [
    "directory = r\"C:\\Users\\bl314\\Box\\CoganLab\\task_stimuli\\LexicalDecRepDelay\"\n",
    "syllables = get_wav_files(directory)\n",
    "for syllable in syllables.keys():\n",
    "    syllables[syllable] = dict.get(syllable)\n",
    "    if syllable=='pilot' and len(syllables[syllable])<5:\n",
    "        syllables[syllable].append('T')\n",
    "    print(f\"{syllable}: {syllables[syllable]}\")\n",
    "syllables_timestamps = create_phoneme_start_time_dict(syllables)\n",
    "syllables_timestamps_DF=pd.DataFrame(syllables_timestamps)\n",
    "print(syllables_timestamps_DF)\n",
    "# Get time of the first phoneme\n",
    "print(np.mean(syllables_timestamps_DF.iloc[1]))\n",
    "print(np.std(syllables_timestamps_DF.iloc[1]))\n",
    "\n",
    "print(np.mean(syllables_timestamps_DF.iloc[2]))\n",
    "print(np.std(syllables_timestamps_DF.iloc[2]))\n",
    "\n",
    "print(syllables.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a33f604e7d93969",
   "metadata": {},
   "source": [
    "#### 1.2.2 Get phoneme proportions for word corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97d772f31379b77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T03:54:46.684980Z",
     "start_time": "2025-10-27T03:54:46.678213Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def split_dictionaries(syllables_dict, lexsts_dict):\n",
    "    syllable_words = {}\n",
    "    syllable_nonwords = {}\n",
    "    \n",
    "    for word, phonemes in syllables_dict.items():\n",
    "        if word in lexsts_dict:\n",
    "            if lexsts_dict[word] == [1]:\n",
    "                syllable_words[word] = phonemes\n",
    "            elif lexsts_dict[word] == [0]:\n",
    "                syllable_nonwords[word] = phonemes\n",
    "                \n",
    "    return syllable_words, syllable_nonwords\n",
    "\n",
    "def calculate_phoneme_proportions(phoneme_dict):\n",
    "    if not phoneme_dict:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    max_len = max(len(p) for p in phoneme_dict.values())\n",
    "    total_words = len(phoneme_dict)\n",
    "    \n",
    "    counts = [defaultdict(int) for _ in range(max_len)]\n",
    "    \n",
    "    for phoneme_list in phoneme_dict.values():\n",
    "        for i, phoneme in enumerate(phoneme_list):\n",
    "            counts[i][phoneme] += 1\n",
    "            \n",
    "    proportions_data = {}\n",
    "    \n",
    "    for i, pos_counts in enumerate(counts):\n",
    "        position = i + 1\n",
    "        proportions_data[position] = {\n",
    "            phoneme: count / total_words \n",
    "            for phoneme, count in pos_counts.items()\n",
    "        }\n",
    "\n",
    "    df = pd.DataFrame(proportions_data).T.fillna(0)\n",
    "    \n",
    "    df.index.name = 'Position'\n",
    "    df = df.reindex(sorted(df.index), axis=0)\n",
    "    df = df.reindex(sorted(df.columns), axis=1)\n",
    "\n",
    "    df = df.round(3) \n",
    "\n",
    "    return df\n",
    "\n",
    "def calculate_similarity(df_words, df_nonwords):\n",
    "    \"\"\"\n",
    "    Calculates the Cosine Similarity for the positional distribution \n",
    "    of each phoneme between the words and non-words dataframes.\n",
    "    \"\"\"\n",
    "    # 1. Align the dataframes on all common phonemes\n",
    "    common_phonemes = sorted(list(set(df_words.columns) & set(df_nonwords.columns)))\n",
    "    \n",
    "    if not common_phonemes:\n",
    "        return pd.Series(dtype=float)\n",
    "\n",
    "    # Reindex both dataframes to ensure they have the same positions (rows) \n",
    "    # and common phonemes (columns)\n",
    "    all_positions = sorted(list(set(df_words.index) | set(df_nonwords.index)))\n",
    "    \n",
    "    # Extract phoneme distribution vectors, filling missing values with 0\n",
    "    words_vectors = df_words.reindex(index=all_positions, columns=common_phonemes).fillna(0)\n",
    "    nonwords_vectors = df_nonwords.reindex(index=all_positions, columns=common_phonemes).fillna(0)\n",
    "\n",
    "    similarity_results = {}\n",
    "\n",
    "    for phoneme in common_phonemes:\n",
    "        # P is the distribution vector for the phoneme in words across all positions\n",
    "        P = words_vectors[phoneme].values.reshape(1, -1)\n",
    "        # Q is the distribution vector for the phoneme in non-words across all positions\n",
    "        Q = nonwords_vectors[phoneme].values.reshape(1, -1)\n",
    "        \n",
    "        # Calculate Cosine Similarity. \n",
    "        # If either P or Q is a zero-vector (phoneme never appears in one set), \n",
    "        # we can skip or handle as 0 similarity (no shared direction).\n",
    "        # We check for a non-zero vector before calculating similarity.\n",
    "        if np.linalg.norm(P) == 0 or np.linalg.norm(Q) == 0:\n",
    "            similarity = 0.0\n",
    "        else:\n",
    "            similarity = cosine_similarity(P, Q)[0][0]\n",
    "        \n",
    "        similarity_results[phoneme] = similarity\n",
    "\n",
    "    # Convert to a Series, sort by similarity (high to low), and round\n",
    "    results_series = pd.Series(similarity_results).sort_values(ascending=False).round(4)\n",
    "    return results_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32af921a495a811",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T03:34:20.845540Z",
     "start_time": "2025-10-21T03:34:20.820847Z"
    }
   },
   "outputs": [],
   "source": [
    "# In the past we tried split word and nonword. But we don't have to now.\n",
    "syllable_words, syllable_nonwords = split_dictionaries(syllables, lexsts_bi_dict)\n",
    "\n",
    "print(\"--- Phoneme Proportions Table for Words (syllable_words) ---\")\n",
    "print(f\"Total Words: {len(syllable_words)}\")\n",
    "df_words = calculate_phoneme_proportions(syllable_words)\n",
    "print(df_words)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"--- Phoneme Proportions Table for Non-Words (syllable_nonwords) ---\")\n",
    "print(f\"Total Non-Words: {len(syllable_nonwords)}\")\n",
    "df_nonwords = calculate_phoneme_proportions(syllable_nonwords)\n",
    "print(df_nonwords)\n",
    "\n",
    "similarity_scores = calculate_similarity(df_words, df_nonwords)\n",
    "print(similarity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96906c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do phoneme comparison for words and nonwords\n",
    "\n",
    "print(syllable_words)\n",
    "print(syllable_nonwords)\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# 1. Count phoneme frequencies for words and nonwords\n",
    "word_phonemes = [phoneme for word in syllable_words.values() for phoneme in word]\n",
    "nonword_phonemes = [phoneme for word in syllable_nonwords.values() for phoneme in word]\n",
    "\n",
    "word_counts = Counter(word_phonemes)\n",
    "nonword_counts = Counter(nonword_phonemes)\n",
    "\n",
    "# 2. Create a DataFrame for display\n",
    "all_phonemes = sorted(list(set(word_counts.keys()) | set(nonword_counts.keys())))\n",
    "freq_df = pd.DataFrame(index=all_phonemes, columns=['Word Frequency', 'Non-word Frequency'])\n",
    "\n",
    "for phoneme in all_phonemes:\n",
    "    freq_df.loc[phoneme, 'Word Frequency'] = word_counts.get(phoneme, 0)\n",
    "    freq_df.loc[phoneme, 'Non-word Frequency'] = nonword_counts.get(phoneme, 0)\n",
    "\n",
    "print(\"Phoneme Frequencies:\\n\")\n",
    "print(freq_df)\n",
    "\n",
    "# 3. Perform Chi-squared test\n",
    "# The contingency table for the chi-squared test will have phonemes as rows\n",
    "# and word type (word/non-word) as columns.\n",
    "contingency_table = freq_df.to_numpy().astype(int) # <-- FIX: Cast to a numeric dtype\n",
    "\n",
    "# Now, the test will run without errors.\n",
    "\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "print(\"\\n--- Chi-squared Test Results ---\")\n",
    "print(f\"Chi-squared statistic: {chi2:.4f}\")\n",
    "print(f\"P-value: {p:.4f}\")\n",
    "print(f\"Degrees of freedom: {dof}\")\n",
    "\n",
    "alpha = 0.05\n",
    "print(f\"\\nSignificance level (alpha): {alpha}\")\n",
    "if p < alpha:\n",
    "    print(\"The p-value is less than the significance level.\")\n",
    "    print(\"We reject the null hypothesis: there is a significant difference in phoneme distributions between words and non-words.\")\n",
    "else:\n",
    "    print(\"The p-value is not less than the significance level.\")\n",
    "    print(\"We fail to reject the null hypothesis: there is no significant difference in phoneme distributions between words and non-words.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49ca9f1fa4766b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Phoneme (Selected) Proportions Table for Words ---\")\n",
    "print(df_words[['AA1', 'AE1', 'AH0', 'EY1', 'EH1', 'IH0']])\n",
    "print(\"                 \")\n",
    "print(\"--- Phoneme (Selected) Proportions Table for Nonwords ---\")\n",
    "print(df_nonwords[['AA1', 'AE1', 'AH0', 'EY1', 'EH1', 'IH0']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbcf128a324f67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T18:41:29.250335Z",
     "start_time": "2025-10-21T18:41:29.244524Z"
    }
   },
   "outputs": [],
   "source": [
    "#For LME: save phoneme categories and timestamps for each syllable\n",
    "import pickle\n",
    "filename = os.path.join(lme_dir,'stim_pho_dict.pkl')\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(syllables, f)\n",
    "filename = os.path.join(lme_dir,'stim_pho_t_dict.pkl')\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(syllables_timestamps, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e839fa48040c348",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T15:14:26.455870Z",
     "start_time": "2025-10-25T15:14:26.448014Z"
    }
   },
   "outputs": [],
   "source": [
    "df_words = calculate_phoneme_proportions(syllables)\n",
    "THRESHOLD = 0.1\n",
    "columns_to_keep = (df_words > THRESHOLD).any(axis=0)\n",
    "filtered_df = df_words.loc[:, columns_to_keep]\n",
    "resulting_column_names = filtered_df.columns.tolist()\n",
    "print(resulting_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a393377ae99f10d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T15:14:30.013209Z",
     "start_time": "2025-10-25T15:14:30.005032Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"--- Phoneme (Selected) Proportions Table for Words ---\")\n",
    "print(df_words[['K', 'L', 'M', 'N', 'R', 'T', 'V']])\n",
    "print(df_words[['AA1', 'AE1', 'AH0', 'AO1', 'EH0', 'EH1', 'EY1', 'IH0']])\n",
    "print(\"                 \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc1600d8644bbeb",
   "metadata": {},
   "source": [
    "### 1.3. (!!!!! OLD VERSION: get all phonemes) One-hot encoding for each unique phoneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecf8a359cd2f974",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T03:54:54.974431Z",
     "start_time": "2025-10-27T03:54:54.968881Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get unique phoneme vector\n",
    "all_phonemes = set()\n",
    "for phonemes in syllables.values():\n",
    "    all_phonemes.update(phonemes)\n",
    "all_phonemes = sorted(list(all_phonemes))  # Sort for consistent ordering\n",
    "phoneme_to_index = {phoneme: idx for idx, phoneme in enumerate(all_phonemes)}\n",
    "vector_length = len(all_phonemes)\n",
    "print(len(all_phonemes))\n",
    "\n",
    "# Create one-hot encoding for each word\n",
    "phoneme_one_hot_dict = {}\n",
    "for word, phonemes in syllables.items():\n",
    "    # Initialize vector with zeros\n",
    "    vector = [0] * vector_length\n",
    "    # Set 1 for each phoneme present in the word\n",
    "    for phoneme in phonemes:\n",
    "        vector[phoneme_to_index[phoneme]] = 1\n",
    "    phoneme_one_hot_dict[word] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f503d6e1e6b450",
   "metadata": {},
   "source": [
    "###  (!!!!! OLD VERSION: get all phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac5189faebf939f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T03:55:01.584838Z",
     "start_time": "2025-10-27T03:54:59.282809Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check the multicolinearity and reduce the feature dimensions\n",
    "check_multicollinearity(np.array(list(phoneme_one_hot_dict.values())),all_phonemes)\n",
    "# Print the words tha contain AE0, W, ER0, and Y\n",
    "for syllable, value in syllables.items():\n",
    "    if value and \"AE0\" in value:\n",
    "        print(f\"{syllable}: {value} contains AE0\")\n",
    "    if value and \"W\" in value:\n",
    "        print(f\"{syllable}: {value} contains W\")\n",
    "    if value and \"ER0\" in value:\n",
    "        print(f\"{syllable}: {value} contains ER0\")\n",
    "    if value and \"Y\" in value:\n",
    "        print(f\"{syllable}: {value} contains Y\")\n",
    "    if value and \"UW1\" in value:\n",
    "        print(f\"{syllable}: {value} contains UW1\")\n",
    "    if value and \"HH\" in value and \"M\" in value:\n",
    "        print(f\"{syllable}: {value} contains both HH and M\")\n",
    "    if value and \"T\" in value and \"L\" in value and \"AH0\" in value:\n",
    "        print(f\"{syllable}: {value} contains T, L and AH0\")\n",
    "    if value and \"OW1\" in value and \"M\" in value and \"N\" in value:\n",
    "        print(f\"{syllable}: {value} contains OW1, M and N\")\n",
    "# I think it is safe to reduce ER0, Y, UW1, AE0, and W\n",
    "# Then: humor - HH and M, tulip - 'T', 'L', 'AH0', 'P', women - 'OW1', 'M', 'N'\n",
    "\n",
    "# reduce the dimensions:\n",
    "indices_to_remove=[all_phonemes.index(\"ER0\"),all_phonemes.index(\"Y\"),all_phonemes.index(\"UW1\"),all_phonemes.index(\"AE0\"),all_phonemes.index(\"W\")]\n",
    "phoneme_one_hot_dict_filt = {key: vec for key, vec in phoneme_one_hot_dict.items()}\n",
    "for i, (key, vec) in enumerate(phoneme_one_hot_dict.items()):\n",
    "    # Create a copy of the vector with specified indices removed\n",
    "    filtered_vec = np.delete(vec, indices_to_remove)\n",
    "    phoneme_one_hot_dict_filt[key] = filtered_vec\n",
    "all_phonemes_filt = []\n",
    "all_phonemes_filt = [p for i, p in enumerate(all_phonemes) if i not in indices_to_remove]\n",
    "\n",
    "# Check the multicolinearity again\n",
    "check_multicollinearity(np.array(list(phoneme_one_hot_dict_filt.values())),all_phonemes_filt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c2dd3cb3264afb",
   "metadata": {},
   "source": [
    "### 1.3 (!!! NEW VERSION: only combine word and nonword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee53d0c52e48b38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T15:14:41.493293Z",
     "start_time": "2025-10-25T15:14:41.159442Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# R\n",
    "# When combine words and nonwords, and removed all the small contributors (proportions < 0.05)\n",
    "# all_phonemes = ['AA1', 'AE1', 'AH0', 'EY1', 'EH1', 'IH0']\n",
    "\n",
    "# When just remove all the small contributors but not separate words and nonwords\n",
    "all_phonemes=['AA1', 'AE1', 'AH0', 'AO1', 'EH0', 'EH1', 'EY1', 'IH0','K', 'L', 'M', 'N', 'R', 'T', 'V']\n",
    "# vowels\n",
    "# all_phonemes=['AA1', 'AE1', 'AH0', 'AO1', 'EH0', 'EH1', 'EY1', 'IH0']\n",
    "# consonants\n",
    "# all_phonemes=['K', 'L', 'M', 'N', 'R', 'T', 'V']\n",
    "all_phonemes = sorted(list(all_phonemes))  # Sort for consistent ordering\n",
    "phoneme_to_index = {phoneme: idx for idx, phoneme in enumerate(all_phonemes)}\n",
    "vector_length = len(all_phonemes)\n",
    "print(phoneme_to_index)\n",
    "# Create one-hot encoding for each word\n",
    "phoneme_one_hot_dict = {}\n",
    "for word, phonemes in syllables.items():\n",
    "    # Initialize vector with zeros\n",
    "    vector = [0] * vector_length\n",
    "    # Set 1 for each phoneme present in the word\n",
    "    for phoneme in phonemes:\n",
    "        if phoneme in all_phonemes:\n",
    "            vector[phoneme_to_index[phoneme]] = 1\n",
    "    phoneme_one_hot_dict[word] = vector\n",
    "check_multicollinearity(np.array(list(phoneme_one_hot_dict.values())),all_phonemes)\n",
    "phoneme_one_hot_dict_filt=phoneme_one_hot_dict\n",
    "all_phonemes_filt=all_phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac23726dd7ff726",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T15:14:50.558317Z",
     "start_time": "2025-10-25T15:14:49.097101Z"
    }
   },
   "outputs": [],
   "source": [
    "# PCA\n",
    "print(\"\\n--- Running PCA with automatic component selection (95% variance) ---\")\n",
    "pca_results_auto, _, loadings_df = perform_pca_on_acoustic_features(\n",
    "    phoneme_one_hot_dict_filt,original_feature_names=all_phonemes_filt,\n",
    "    n_components=None, # Let the function decide\n",
    "    min_variance_retention=0.9\n",
    ")\n",
    "phoneme_one_hot_dict_filt_pca = pca_results_auto.to_dict('index')\n",
    "phoneme_one_hot_dict_filt_pca = {key: list(value.values()) for key, value in phoneme_one_hot_dict_filt_pca.items()}\n",
    "sns.heatmap(loadings_df)\n",
    "\n",
    "for key, values in list(phoneme_one_hot_dict_filt_pca.items())[:4]:  # Show first 5 entries\n",
    "    print(key, \":\", values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4056e594099a3dda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T03:55:32.600965Z",
     "start_time": "2025-10-27T03:55:32.581838Z"
    }
   },
   "outputs": [],
   "source": [
    "# save the phoneme_one_hot_dicphoneme_one_hot_dictt\n",
    "import pickle\n",
    "phoneme_one_hot_dict_path = \"phoneme_one_hot_dict.pickle\"\n",
    "with open(phoneme_one_hot_dict_path, 'wb') as handle:\n",
    "    pickle.dump(phoneme_one_hot_dict_filt, handle)\n",
    "phoneme_one_hot_dict_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ecc226fb1d626c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T03:55:36.267733Z",
     "start_time": "2025-10-27T03:55:36.254155Z"
    }
   },
   "outputs": [],
   "source": [
    "range(all_phonemes_filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c58caf0c19a5ca0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T03:58:10.988640Z",
     "start_time": "2025-10-27T03:58:10.968086Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "phoneme_one_hot_dict_filt_df = pd.DataFrame(phoneme_one_hot_dict_filt)\n",
    "phoneme_one_hot_dict_filt_df.index = [f\"pho{i+1}\" for i in range(len(all_phonemes_filt))]\n",
    "phoneme_one_hot_dict_filt_df.to_csv(os.path.join(lme_dir,'phoneme_one_hot_dict.csv'), index=True)\n",
    "phoneme_one_hot_dict_filt_pca_df = pd.DataFrame(phoneme_one_hot_dict_filt_pca)\n",
    "num_rows = phoneme_one_hot_dict_filt_pca_df.shape[0]\n",
    "phoneme_one_hot_dict_filt_pca_df.index = [f\"pho{i+1}\" for i in range(num_rows)]\n",
    "print(phoneme_one_hot_dict_filt_pca_df)\n",
    "phoneme_one_hot_dict_filt_pca_df.to_csv(os.path.join(lme_dir,'phoneme_one_hot_dict_pca.csv'), index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d40eb0a2ff5ab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read the saved one hot dict. Just for demonstration\n",
    "# import pandas as pd\n",
    "# d= pd.read_pickle(\"phoneme_one_hot_dict.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160da43c",
   "metadata": {},
   "source": [
    "### 1.4. Semantic features using GloVE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d232013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GloVE stuff\n",
    "def load_glove_manual(glove_file):\n",
    "    embeddings_dict = {}\n",
    "    \n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            try:\n",
    "                vector = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_dict[word] = vector\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "    print(f\"加载完成！共包含 {len(embeddings_dict)} 个词向量。\")\n",
    "    return embeddings_dict\n",
    "\n",
    "\n",
    "glove_path = os.path.join(\"..\",\"..\",\"..\",\"GLoVE\",\"dolma_300_2024_1.2M.100_combined.txt\") \n",
    "glove_vectors = load_glove_manual(glove_path)\n",
    "\n",
    "# example usage\n",
    "word = \"brain\"\n",
    "if word in glove_vectors:\n",
    "    vec = glove_vectors[word]\n",
    "    print(f\"单词: {word}\")\n",
    "    print(f\"维度: {vec.shape}\") # 应该是 (300,)\n",
    "    print(f\"前10位数值: {vec[:10]}\")\n",
    "else:\n",
    "    print(\"词表中没有这个词\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564864c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = r\"C:\\Users\\bl314\\Box\\CoganLab\\task_stimuli\\LexicalDecRepDelay\"\n",
    "syllables_vectors = get_wav_files(directory)\n",
    "# syllable_words, syllable_nonwords = split_dictionaries(syllables, lexsts_bi_dict)\n",
    "syllables_sem = {\n",
    "    word: glove_vectors[word] \n",
    "    for word in syllables_vectors.keys() \n",
    "    if word in glove_vectors\n",
    "}\n",
    "print(syllables_sem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aadb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wordfreq \n",
    "\n",
    "# ==============================================================================\n",
    "# UPDATED SEARCH LOGIC: SPECIFIC PHONOLOGICAL CONSTRAINTS\n",
    "# \n",
    "# Helper: Vowel detection (ARPABET vowels end in digits, e.g., 'AA1').\n",
    "# \n",
    "# Priority 1: \n",
    "#   - Match first 3 phonemes (indices 0, 1, 2).\n",
    "#   - Constraint: Candidate's 4th phoneme (index 3) must be a VOWEL.\n",
    "# \n",
    "# Priority 2: \n",
    "#   - Match first 2 phonemes (indices 0, 1).\n",
    "#   - Constraint: Candidate's 3rd phoneme (index 2) must be a CONSONANT.\n",
    "# \n",
    "# Selection: Highest frequency word satisfying the rule.\n",
    "# ==============================================================================\n",
    "\n",
    "def is_vowel(phoneme_str):\n",
    "    \"\"\"\n",
    "    Determines if a phoneme is a vowel based on ARPABET convention.\n",
    "    Vowels end with a stress digit (e.g., 'AA1', 'IY0').\n",
    "    Consonants do not (e.g., 'K', 'S', 'CH').\n",
    "    \"\"\"\n",
    "    if not phoneme_str:\n",
    "        return False\n",
    "    # Check if the last character is a digit\n",
    "    return phoneme_str[-1].isdigit()\n",
    "\n",
    "def is_consonant(phoneme_str):\n",
    "    \"\"\"Determines if a phoneme is a consonant.\"\"\"\n",
    "    return not is_vowel(phoneme_str)\n",
    "\n",
    "def get_frequency(word, lang='en'):\n",
    "    \"\"\"Returns Zipf frequency of a word.\"\"\"\n",
    "    return wordfreq.zipf_frequency(word, lang)\n",
    "\n",
    "def build_common_word_cache(glove_vectors, pronunciation_dict, threshold=3.0):\n",
    "    \"\"\"\n",
    "    Builds a cache of real words sorted by frequency.\n",
    "    Threshold 3.0 excludes rare/archaic words (like 'causeth').\n",
    "    \"\"\"\n",
    "    print(\"Building high-frequency word cache...\")\n",
    "    cache = []\n",
    "    \n",
    "    for word in glove_vectors.keys():\n",
    "        if word in pronunciation_dict:\n",
    "            freq = get_frequency(word)\n",
    "            if freq > threshold:\n",
    "                cache.append({\n",
    "                    'word': word,\n",
    "                    'phonemes': tuple(pronunciation_dict[word]),\n",
    "                    'freq': freq\n",
    "                })\n",
    "    \n",
    "    # Sort descending by frequency. \n",
    "    # This ensures the first match found in the loop is the \"Highest Frequency\" one.\n",
    "    cache.sort(key=lambda x: x['freq'], reverse=True)\n",
    "    \n",
    "    print(f\"Cached {len(cache)} common words for searching.\")\n",
    "    return cache\n",
    "\n",
    "def find_constrained_cohort_proxy(nonword, pronunciation_dict, common_word_cache):\n",
    "    \"\"\"\n",
    "    Finds a proxy based on the specific Vowel/Consonant rules at specific positions.\n",
    "    \"\"\"\n",
    "    # 1. Get Nonword Phonemes\n",
    "    if nonword not in pronunciation_dict:\n",
    "        print(f\"  [Warning] No pronunciation data for: {nonword}\")\n",
    "        return None\n",
    "        \n",
    "    nw_phonemes = tuple(pronunciation_dict[nonword])\n",
    "    nw_len = len(nw_phonemes)\n",
    "    \n",
    "    # =========================================================\n",
    "    # PRIORITY 1: Match 3 Phonemes + Pos 4 is VOWEL\n",
    "    # =========================================================\n",
    "    # We need the nonword to have at least 3 phonemes to define the prefix\n",
    "    if nw_len >= 3:\n",
    "        prefix_3 = nw_phonemes[:3] # Indices 0, 1, 2\n",
    "        \n",
    "        for entry in common_word_cache:\n",
    "            cand_phonemes = entry['phonemes']\n",
    "            \n",
    "            # Rule Check:\n",
    "            # 1. Candidate must be long enough (at least 4 phonemes)\n",
    "            # 2. Prefix (0,1,2) must match\n",
    "            # 3. Pos 4 (index 3) must be a VOWEL\n",
    "            if (len(cand_phonemes) >= 4 and \n",
    "                cand_phonemes[:3] == prefix_3 and \n",
    "                is_vowel(cand_phonemes[3])):\n",
    "                \n",
    "                print(f\"  [Match P1 (3+Vowel)] {nonword} ({prefix_3}) -> {entry['word']} (Pos4: {cand_phonemes[3]})\")\n",
    "                return entry['word']\n",
    "\n",
    "    # =========================================================\n",
    "    # PRIORITY 2: Match 2 Phonemes + Pos 3 is CONSONANT\n",
    "    # =========================================================\n",
    "    # We need the nonword to have at least 2 phonemes\n",
    "    if nw_len >= 2:\n",
    "        prefix_2 = nw_phonemes[:2] # Indices 0, 1\n",
    "        \n",
    "        for entry in common_word_cache:\n",
    "            cand_phonemes = entry['phonemes']\n",
    "            \n",
    "            # Rule Check:\n",
    "            # 1. Candidate must be long enough (at least 3 phonemes)\n",
    "            # 2. Prefix (0,1) must match\n",
    "            # 3. Pos 3 (index 2) must be a CONSONANT\n",
    "            if (len(cand_phonemes) >= 3 and \n",
    "                cand_phonemes[:2] == prefix_2 and \n",
    "                is_consonant(cand_phonemes[2])):\n",
    "                \n",
    "                print(f\"  [Match P2 (2+Cons)] {nonword} ({prefix_2}) -> {entry['word']} (Pos3: {cand_phonemes[2]})\")\n",
    "                return entry['word']\n",
    "\n",
    "    # =========================================================\n",
    "    # FAILURE\n",
    "    # =========================================================\n",
    "    print(f\"  [Failure] No constrained match found for {nonword}\")\n",
    "    return None\n",
    "\n",
    "# ================= Main Execution =================\n",
    "\n",
    "# 1. Build Cache (Run once)\n",
    "# Using threshold=3.0 to ensure we only get \"High Frequency\" words\n",
    "common_word_cache = build_common_word_cache(glove_vectors, dict, threshold=3.0)\n",
    "\n",
    "syllables_sem_proxy = {}\n",
    "real_words_found = set()\n",
    "proxy_pairs_map = {} \n",
    "\n",
    "print(\"\\nProcessing stimuli...\")\n",
    "for word in syllables_vectors.keys():\n",
    "    \n",
    "    # 1. Real Word Check\n",
    "    if word in glove_vectors:\n",
    "        syllables_sem_proxy[word] = glove_vectors[word]\n",
    "        real_words_found.add(word)\n",
    "        \n",
    "    # 2. Nonword Check\n",
    "    else:\n",
    "        print(f\"Searching proxy for: {word}\")\n",
    "        \n",
    "        proxy_word = find_constrained_cohort_proxy(\n",
    "            word, \n",
    "            dict, \n",
    "            common_word_cache\n",
    "        )\n",
    "        \n",
    "        if proxy_word:\n",
    "            syllables_sem_proxy[word] = glove_vectors[proxy_word]\n",
    "            proxy_pairs_map[word] = proxy_word\n",
    "        else:\n",
    "            # If no proxy found, you might want to leave it out or fill 0 later\n",
    "            pass\n",
    "print(proxy_pairs_map)\n",
    "\n",
    "# Optional: Visualize results\n",
    "# if len(syllables_sem_proxy) > 5:\n",
    "#     plot_words_with_proxies(syllables_sem_proxy, real_words_found, proxy_pairs_map, method='t-SNE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1dc087",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [str(i) for i in range(300)]\n",
    "# Check the multicolinearity again\n",
    "check_multicollinearity(np.array(list(syllables_sem.values())),col_names)\n",
    "check_multicollinearity(np.array(list(syllables_sem_proxy.values())),col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf77dd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA: semantic vectors\n",
    "print(\"\\n--- Running PCA with automatic component selection (95% variance) ---\")\n",
    "pca_results_auto, _, loadings_df = perform_pca_on_acoustic_features(\n",
    "    syllables_sem,original_feature_names=col_names,\n",
    "    n_components=None, # Let the function decide\n",
    "    min_variance_retention=0.9\n",
    ")\n",
    "syllables_sem_pca = pca_results_auto.to_dict('index')\n",
    "syllables_sem_pca = {key: list(value.values()) for key, value in syllables_sem_pca.items()}\n",
    "sns.heatmap(loadings_df)\n",
    "\n",
    "for key, values in list(syllables_sem_pca.items())[:4]:  # Show first 5 entries\n",
    "    print(key, \":\", values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcfae3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA: semantic vectors + proxies\n",
    "print(\"\\n--- Running PCA with automatic component selection (95% variance) ---\")\n",
    "pca_results_auto, _, loadings_df = perform_pca_on_acoustic_features(\n",
    "    syllables_sem_proxy,original_feature_names=col_names,\n",
    "    n_components=None, # Let the function decide\n",
    "    min_variance_retention=0.9\n",
    ")\n",
    "syllables_sem_proxy_pca = pca_results_auto.to_dict('index')\n",
    "syllables_sem_proxy_pca = {key: list(value.values()) for key, value in syllables_sem_proxy_pca.items()}\n",
    "sns.heatmap(loadings_df)\n",
    "\n",
    "for key, values in list(syllables_sem_proxy_pca.items())[:4]:  # Show first 5 entries\n",
    "    print(key, \":\", values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a250c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_words_with_proxies(word_vector_dict, real_words_set, proxy_mapping, method='t-SNE'):\n",
    "    \"\"\"\n",
    "    Robust plotting function that strictly separates Real Words and Nonwords.\n",
    "    \"\"\"\n",
    "    # --- 1. 数据完整性检查 (Debug Info) ---\n",
    "    all_words = list(word_vector_dict.keys())\n",
    "    vectors = np.array(list(word_vector_dict.values()))\n",
    "    \n",
    "    # 找出哪些是真词，哪些是非词\n",
    "    # 逻辑：如果这个词在 vector_dict 里，但不在 real_words_set 里，它就是非词\n",
    "    real_indices = []\n",
    "    proxy_indices = []\n",
    "    \n",
    "    for i, word in enumerate(all_words):\n",
    "        if word in real_words_set:\n",
    "            real_indices.append(i)\n",
    "        else:\n",
    "            proxy_indices.append(i)\n",
    "            \n",
    "    print(f\"--- Plotting Debug Info ---\")\n",
    "    print(f\"Total words passed to plot: {len(all_words)}\")\n",
    "    print(f\"Identified as Real Words (Black): {len(real_indices)}\")\n",
    "    print(f\"Identified as Nonwords (Blue): {len(proxy_indices)}\")\n",
    "    \n",
    "    if len(proxy_indices) == 0:\n",
    "        print(\"WARNING: No nonwords identified! Check if 'word_vector_dict' actually contains the nonword keys.\")\n",
    "        # 如果这里是 0，说明主循环里没把非词加进 syllables_sem_proxy\n",
    "        \n",
    "    if len(vectors) < 2:\n",
    "        print(\"Not enough data to plot.\")\n",
    "        return\n",
    "\n",
    "    # --- 2. 降维 (Dimensionality Reduction) ---\n",
    "    print(f\"Running {method}...\")\n",
    "    if method == 't-SNE':\n",
    "        perp = min(30, len(vectors) - 1)\n",
    "        reducer = TSNE(n_components=2, random_state=42, perplexity=perp, init='pca', learning_rate='auto')\n",
    "        coords = reducer.fit_transform(vectors)\n",
    "    else:\n",
    "        reducer = PCA(n_components=2)\n",
    "        coords = reducer.fit_transform(vectors)\n",
    "\n",
    "    # --- 3. 分批绘图 (Batch Plotting) ---\n",
    "    plt.figure(figsize=(14, 12), dpi=100)\n",
    "    \n",
    "    # 3.1 画背景淡点 (Ghost points for context)\n",
    "    plt.scatter(coords[:, 0], coords[:, 1], c='lightgray', s=30, alpha=0.2)\n",
    "\n",
    "    # 3.2 批量画真词 (Layer 1: Real Words - Black)\n",
    "    if real_indices:\n",
    "        real_coords = coords[real_indices]\n",
    "        plt.scatter(real_coords[:, 0], real_coords[:, 1], \n",
    "                   c='black', s=60, alpha=0.6, label='Real Word')\n",
    "        \n",
    "        # 标注真词\n",
    "        for idx in real_indices:\n",
    "            word = all_words[idx]\n",
    "            x, y = coords[idx]\n",
    "            plt.annotate(word, (x, y), xytext=(3, 3), textcoords='offset points', \n",
    "                         fontsize=10, color='black', alpha=0.8)\n",
    "\n",
    "    # 3.3 批量画非词 (Layer 2: Nonwords - Blue - On Top)\n",
    "    if proxy_indices:\n",
    "        proxy_coords = coords[proxy_indices]\n",
    "        plt.scatter(proxy_coords[:, 0], proxy_coords[:, 1], \n",
    "                   c='blue', s=100, alpha=1.0, edgecolors='white', linewidth=1, label='Nonword (Proxy)')\n",
    "        \n",
    "        # 标注非词\n",
    "        for idx in proxy_indices:\n",
    "            word = all_words[idx] # 这是非词本身的拼写 (如 'bep')\n",
    "            proxy_target = proxy_mapping.get(word, \"??\") # 这是替身 (如 'best')\n",
    "            \n",
    "            label_text = f\"{word}\\n({proxy_target})\"\n",
    "            x, y = coords[idx]\n",
    "            \n",
    "            # 使用箭头指向，防止文字重叠\n",
    "            plt.annotate(label_text, (x, y), xytext=(10, 10), textcoords='offset points',\n",
    "                         fontsize=11, color='blue', fontweight='bold',\n",
    "                         arrowprops=dict(arrowstyle=\"-\", color='blue', alpha=0.5))\n",
    "\n",
    "    # --- 4. 装饰 ---\n",
    "    plt.title(f\"Semantic Mapping: Nonwords to Real-Word Proxies ({method})\", fontsize=15)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ==========================================\n",
    "# 运行检查\n",
    "# ==========================================\n",
    "# 确保 syllables_sem_proxy 里既有真词的key，也有非词的key\n",
    "pronounce_dict=dict\n",
    "del dict\n",
    "if len(syllables_sem_proxy) > 0:\n",
    "    plot_words_with_proxies(\n",
    "        syllables_sem_proxy,    # 必须包含所有你想画的词\n",
    "        real_words_found,       # 真词集合\n",
    "        proxy_pairs_map,        # 非词->替身 映射\n",
    "        method='t-SNE' if len(syllables_sem_proxy) > 5 else 'PCA'\n",
    "    )\n",
    "else:\n",
    "    print(\"Error: The input dictionary 'syllables_sem_proxy' is empty!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeeccc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# semantic vectors\n",
    "syllables_sem_pca_df = pd.DataFrame(syllables_sem_pca)\n",
    "num_rows = syllables_sem_pca_df.shape[0]\n",
    "syllables_sem_pca_df.index = [f\"sem{i+1}\" for i in range(num_rows)]\n",
    "print(syllables_sem_pca_df)\n",
    "syllables_sem_pca_df.to_csv(os.path.join(lme_dir,'syllables_sem_pca.csv'), index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0b8336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic vectors + proxies\n",
    "syllables_sem_proxy_pca_df = pd.DataFrame(syllables_sem_proxy_pca)\n",
    "num_rows = syllables_sem_proxy_pca_df.shape[0]\n",
    "syllables_sem_proxy_pca_df.index = [f\"sem{i+1}\" for i in range(num_rows)]\n",
    "print(syllables_sem_proxy_pca_df)\n",
    "syllables_sem_proxy_pca_df.to_csv(os.path.join(lme_dir,'syllables_sem_proxy_pca.csv'), index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34373c32f714de93",
   "metadata": {},
   "source": [
    "## 2. Acoustic feature (Power envelope at five frequency bins)\n",
    "### 2.1. Read normolized binned envelope \n",
    "(The matrix was generated by `get_stims_envelope.m`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cdba23da27aeaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T04:00:39.517446Z",
     "start_time": "2025-10-27T04:00:39.509531Z"
    }
   },
   "outputs": [],
   "source": [
    "envelope_feature_dict = {}\n",
    "\n",
    "with open(\"envelope_power_bins.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        key = parts[0]\n",
    "        values = list(map(float, parts[1:]))  # Convert remaining columns to float\n",
    "        envelope_feature_dict[key] = values\n",
    "feature_names=['50.00 Hz','120.58 Hz','209.04 Hz','319.88 Hz','458.78 Hz','632.84 Hz','850.96 Hz','1124.30 Hz','1466.84 Hz','1896.08 Hz','2433.98 Hz','3108.04 Hz','3952.74 Hz','5011.26 Hz','6337.74 Hz','8000.00 Hz']\n",
    "# Print a sample of the dictionary\n",
    "for key, values in list(envelope_feature_dict.items())[:5]:  # Show first 5 entries\n",
    "    print(key, \":\", values)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de1e3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See whether words and nonwords are different acoustically\n",
    "directory = r\"C:\\Users\\bl314\\Box\\CoganLab\\task_stimuli\\LexicalDecRepDelay\"\n",
    "syllables_aco = get_wav_files(directory)\n",
    "for syllable in syllables_aco.keys():\n",
    "    syllables_aco[syllable] = envelope_feature_dict.get(syllable)\n",
    "syllables_aco_words, syllables_aco_nonwords = split_dictionaries(syllables_aco, lexsts_bi_dict)\n",
    "print(syllables_aco_nonwords)\n",
    "print(syllables_aco_words)\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# 2. Convert dictionaries to pandas DataFrames for easier handling\n",
    "words_df = pd.DataFrame.from_dict(syllables_aco_words, orient='index', columns=feature_names)\n",
    "nonwords_df = pd.DataFrame.from_dict(syllables_aco_nonwords, orient='index', columns=feature_names)\n",
    "\n",
    "\n",
    "# 3. Perform t-test for each feature/position\n",
    "results = []\n",
    "for feature in feature_names:\n",
    "    # Get the data for the current position for both groups\n",
    "    word_values = words_df[feature]\n",
    "    nonword_values = nonwords_df[feature]\n",
    "    \n",
    "    # Perform independent samples t-test\n",
    "    t_stat, p_value = ttest_ind(word_values, nonword_values, nan_policy='omit')\n",
    "    \n",
    "    results.append({\n",
    "        'Feature': feature,\n",
    "        'T-statistic': t_stat,\n",
    "        'P-value': p_value\n",
    "    })\n",
    "\n",
    "# 4. Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# 5. Apply FDR correction for multiple comparisons\n",
    "alpha = 0.05\n",
    "p_values = results_df['P-value']\n",
    "reject, p_corrected, _, _ = multipletests(p_values, alpha=alpha, method='fdr_bh')\n",
    "\n",
    "# Add corrected results to the DataFrame\n",
    "results_df['P-value (FDR corrected)'] = p_corrected\n",
    "results_df['Significant (uncorrected)'] = results_df['P-value'] < alpha\n",
    "results_df['Significant (FDR corrected)'] = reject\n",
    "\n",
    "\n",
    "# 6. Display and interpret the final results\n",
    "print(\"T-test Results for Each Feature (Words vs. Nonwords) with FDR Correction:\\n\")\n",
    "print(results_df.to_string())\n",
    "\n",
    "print(f\"\\n--- Interpretation (at alpha = {alpha}) ---\\n\")\n",
    "significant_features = results_df[results_df['Significant (FDR corrected)']]\n",
    "\n",
    "if not significant_features.empty:\n",
    "    print(\"After FDR correction, a significant difference was found for the following features:\")\n",
    "    for feature in significant_features['Feature']:\n",
    "        print(f\"- {feature}\")\n",
    "else:\n",
    "    print(\"After FDR correction, no significant difference was found for any of the features.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7f2013a642d765",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T04:03:15.641292Z",
     "start_time": "2025-10-27T04:03:15.631808Z"
    }
   },
   "outputs": [],
   "source": [
    "envelope_feature_dict_df = pd.DataFrame(envelope_feature_dict)\n",
    "envelope_feature_dict_df.index = [f\"aco{i+1}\" for i in range(len(feature_names))]\n",
    "envelope_feature_dict_df.to_csv(os.path.join(lme_dir,'envelope_feature_dict.csv'), index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57a46752f7686b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check the multicolinearity and reduce the feature dimensions\n",
    "check_multicollinearity(np.array(list(envelope_feature_dict.values())),\n",
    "                        feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153453573d37ec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "print(\"\\n--- Running PCA with automatic component selection (95% variance) ---\")\n",
    "pca_results_auto, _, loadings_df = perform_pca_on_acoustic_features(\n",
    "    envelope_feature_dict,original_feature_names=feature_names,\n",
    "    n_components=None, # Let the function decide\n",
    "    min_variance_retention=0.9\n",
    ")\n",
    "sns.heatmap(loadings_df)\n",
    "envelope_feature_dict_pca = pca_results_auto.to_dict('index')\n",
    "envelope_feature_dict_pca = {key: list(value.values()) for key, value in envelope_feature_dict_pca.items()}\n",
    "\n",
    "for key, values in list(envelope_feature_dict_pca.items())[:5]:  # Show first 5 entries\n",
    "    print(key, \":\", values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabf8032a3dcc125",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_multicollinearity(np.array(list(envelope_feature_dict_pca.values())),\n",
    "                        ['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2926683ad6d0bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dictionary\n",
    "import pickle\n",
    "envelope_feature_dict_path = \"envelope_feature_dict.pickle\"\n",
    "with open(envelope_feature_dict_path, 'wb') as handle:\n",
    "    pickle.dump(envelope_feature_dict_pca, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ed364b36bc5d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "envelope_feature_dict_pca = pd.DataFrame(envelope_feature_dict_pca)\n",
    "num_rows = envelope_feature_dict_pca.shape[0]\n",
    "envelope_feature_dict_pca.index = [f\"aco{i+1}\" for i in range(num_rows)]\n",
    "print(envelope_feature_dict_pca)\n",
    "envelope_feature_dict_pca.to_csv(os.path.join(lme_dir,'envelope_feature_dict_pca.csv'), index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63130fbc5b907a69",
   "metadata": {},
   "source": [
    "## 3. Acoustic+Phonemeic feature (for a whole token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df037e41f1f8e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_pca(loadings_df, ratio_threshold=6/4):\n",
    "    hz_cols = [col for col in loadings_df.columns if 'Hz' in col]\n",
    "    other_cols = [col for col in loadings_df.columns if 'Hz' not in col]\n",
    "    \n",
    "    if len(hz_cols) == 0 or len(other_cols) == 0:\n",
    "        raise ValueError(\"Loadings DataFrame must contain at least one 'Hz' feature column and one non-'Hz' feature column.\")\n",
    "\n",
    "    pc_new_names = {}\n",
    "    \n",
    "    aco_counter = 1\n",
    "    pho_counter = 1\n",
    "    acpho_counter = 1\n",
    "\n",
    "    for pc_idx, row in loadings_df.iterrows():\n",
    "        aco_w = np.mean(np.abs(row[hz_cols]))\n",
    "        pho_w = np.mean(np.abs(row[other_cols]))\n",
    "\n",
    "        if pho_w / aco_w > ratio_threshold:\n",
    "            new_name = f'pho{pho_counter}'\n",
    "            pho_counter += 1\n",
    "        elif aco_w / pho_w > ratio_threshold:\n",
    "            new_name = f'aco{aco_counter}'\n",
    "            aco_counter += 1\n",
    "        else:\n",
    "            new_name = f'mix{acpho_counter}'\n",
    "            acpho_counter += 1\n",
    "            \n",
    "        pc_new_names[pc_idx] = new_name\n",
    "    \n",
    "    return pc_new_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b988f4a37ca3784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dictionary of HG responses and make a lexical status mapping\n",
    "HOME = os.path.expanduser(\"~\")\n",
    "LAB_root = os.path.join(HOME, \"Box\", \"CoganLab\")\n",
    "\n",
    "stats_root_delay = os.path.join(LAB_root, 'BIDS-1.0_LexicalDecRepDelay', 'BIDS', \"derivatives\", \"stats\")\n",
    "stats_root_nodelay = os.path.join(LAB_root, 'BIDS-1.0_LexicalDecRepNoDelay', 'BIDS', \"derivatives\", \"stats\")\n",
    "\n",
    "epoc_LexDelayRep_Aud,_=gp.load_stats('zscore','Auditory_inRep','epo',stats_root_delay,stats_root_delay,keeptrials=True,testsubj=True)\n",
    "\n",
    "stim_lexsts = epoc_LexDelayRep_Aud.labels[0].split('/')[:,[3,2]]\n",
    "del epoc_LexDelayRep_Aud\n",
    "lexsts_dict = {str(item[0].split('-')[0]): str(item[1]) for item in stim_lexsts}\n",
    "lexsts_bi_dict = {key: [1] if value == 'Word' else [0] for key, value in lexsts_dict.items()}\n",
    "lexsts_bi_dict['magic']=[1]\n",
    "print(lexsts_bi_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cb9eb9eeb6d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aco_pho_lex_dict = {key: lexsts_bi_dict[key] + envelope_feature_dict[key] + phoneme_one_hot_dict_filt[key].tolist()  for key in envelope_feature_dict}\n",
    "check_multicollinearity(np.array(list(aco_pho_lex_dict.values())),\n",
    "                        ['lex']+feature_names+all_phonemes_filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73dc20a976f5205",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_results_aco_pho, _, loadings_df = perform_pca_on_acoustic_features(\n",
    "    aco_pho_lex_dict,original_feature_names=['lex']+feature_names+all_phonemes_filt,\n",
    "    n_components=None, # Let the function decide\n",
    "    min_variance_retention=0.9\n",
    ")\n",
    "aco_pho_dict_pca = pca_results_aco_pho.to_dict('index')\n",
    "aco_pho_dict_dict_pca = {key: list(value.values()) for key, value in aco_pho_dict_pca.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eef80aa41bb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aco_pho_dict_pca_df = pd.DataFrame(aco_pho_dict_pca)\n",
    "print(aco_pho_dict_pca_df)\n",
    "pc_new_names=classify_pca(loadings_df,ratio_threshold=6/4)\n",
    "aco_pho_dict_pca_df.rename(index=pc_new_names, inplace=True)\n",
    "print(aco_pho_dict_pca_df)\n",
    "sns.heatmap(loadings_df.rename(index=pc_new_names))\n",
    "print(loadings_df.T)\n",
    "aco_pho_dict_pca_df.to_csv(os.path.join(lme_dir,'aco_pho_dict_pca.csv'), index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb94cecb8e872edb",
   "metadata": {},
   "source": [
    "## 4. Acoustic+Phonemeic feature (pho1 or 2 only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43ec701ce1d77b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pho=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da24775a2e60100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get acoustic features\n",
    "phoX_aco_dict = {}\n",
    "with open(f\"envelope_power_bins_pho{pho}.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        key = parts[0]\n",
    "        values = list(map(float, parts[1:]))  # Convert remaining columns to float\n",
    "        phoX_aco_dict[key] = values\n",
    "feature_names=['50.00 Hz','120.58 Hz','209.04 Hz','319.88 Hz','458.78 Hz','632.84 Hz','850.96 Hz','1124.30 Hz','1466.84 Hz','1896.08 Hz','2433.98 Hz','3108.04 Hz','3952.74 Hz','5011.26 Hz','6337.74 Hz','8000.00 Hz']\n",
    "# Print a sample of the dictionary\n",
    "for key, values in list(phoX_aco_dict.items())[:5]:  # Show first 5 entries\n",
    "    print(key, \":\", values)\n",
    "check_multicollinearity(np.array(list(phoX_aco_dict.values())),\n",
    "                        feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43569217ff900f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get phoneme vector for phoX\n",
    "all_phonemes_phoX = set()\n",
    "for phonemes in syllables.values():\n",
    "    all_phonemes_phoX.add(phonemes[pho-1])\n",
    "print(all_phonemes_phoX)\n",
    "all_phonemes_phoX = sorted(list(all_phonemes_phoX))  # Sort for consistent ordering\n",
    "phoneme_to_index = {phoneme: idx for idx, phoneme in enumerate(all_phonemes_phoX)}\n",
    "vector_length = len(all_phonemes_phoX)\n",
    "\n",
    "# Create one-hot encoding for each word (only pho1)\n",
    "phoX_pho_dict = {}\n",
    "for word, phonemes in syllables.items():\n",
    "    # Initialize vector with zeros\n",
    "    vector = [0] * vector_length\n",
    "    # Set 1 for each phoneme present in the word\n",
    "    vector[phoneme_to_index[phonemes[pho-1]]] = 1\n",
    "    phoX_pho_dict[word] = vector\n",
    "\n",
    "check_multicollinearity(np.array(list(phoX_pho_dict.values())),\n",
    "                        all_phonemes_phoX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1b7b382d480729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the two and run PCA\n",
    "phoX_aco_pho_dict = {key: phoX_aco_dict[key] + phoX_pho_dict[key]\n",
    "                    for key in phoX_aco_dict}\n",
    "check_multicollinearity(np.array(list(phoX_aco_pho_dict.values())),\n",
    "                        feature_names+all_phonemes_phoX)\n",
    "phoX_aco_pho_dict_pca_df, _, loadings_df = perform_pca_on_acoustic_features(\n",
    "    phoX_aco_pho_dict,original_feature_names=feature_names+all_phonemes_phoX,\n",
    "    n_components=None, # Let the function decide\n",
    "    min_variance_retention=0.9\n",
    ")\n",
    "print(loadings_df)\n",
    "phoX_aco_pho_dict_pca_df=phoX_aco_pho_dict_pca_df.T\n",
    "print(phoX_aco_pho_dict_pca_df)\n",
    "pc_new_names=classify_pca(loadings_df,ratio_threshold=6/4)\n",
    "phoX_aco_pho_dict_pca_df.rename(index=pc_new_names, inplace=True)\n",
    "print(phoX_aco_pho_dict_pca_df)\n",
    "loadings_df=np.abs(loadings_df)\n",
    "sns.heatmap(loadings_df.rename(index=pc_new_names))\n",
    "phoX_aco_pho_dict_pca_df.to_csv(os.path.join(lme_dir,f'pho{pho}_aco_pho_dict_pca.csv'), index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0c5de5ed097ae6",
   "metadata": {},
   "source": [
    "## 5. Acoustic+Phonemeic feature (syllable1 only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e932d24762316394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get acoustic features\n",
    "syl1_aco_dict = {}\n",
    "with open(\"envelope_power_bins_syl1.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        key = parts[0]\n",
    "        values = list(map(float, parts[1:]))  # Convert remaining columns to float\n",
    "        syl1_aco_dict[key] = values\n",
    "feature_names=['50.00 Hz','120.58 Hz','209.04 Hz','319.88 Hz','458.78 Hz','632.84 Hz','850.96 Hz','1124.30 Hz','1466.84 Hz','1896.08 Hz','2433.98 Hz','3108.04 Hz','3952.74 Hz','5011.26 Hz','6337.74 Hz','8000.00 Hz']\n",
    "# Print a sample of the dictionary\n",
    "for key, values in list(syl1_aco_dict.items())[:5]:  # Show first 5 entries\n",
    "    print(key, \":\", values)\n",
    "check_multicollinearity(np.array(list(syl1_aco_dict.values())),\n",
    "                        feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae50f243704a131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get phoneme vector for pho1\n",
    "all_phonemes_syl1 = set()\n",
    "for phonemes in syllables.values():\n",
    "    all_phonemes_syl1.add(phonemes[0])\n",
    "    all_phonemes_syl1.add(phonemes[1])\n",
    "all_phonemes_syl1 = sorted(list(all_phonemes_syl1))  # Sort for consistent ordering\n",
    "phoneme_to_index = {phoneme: idx for idx, phoneme in enumerate(all_phonemes_syl1)}\n",
    "vector_length = len(all_phonemes_syl1)\n",
    "\n",
    "# Create one-hot encoding for each word (only pho1)\n",
    "syl1_pho_dict = {}\n",
    "for word, phonemes in syllables.items():\n",
    "    # Initialize vector with zeros\n",
    "    vector = [0] * vector_length\n",
    "    # Set 1 for each phoneme present in the word\n",
    "    vector[phoneme_to_index[phonemes[0]]] = 1\n",
    "    vector[phoneme_to_index[phonemes[1]]] = 1\n",
    "    syl1_pho_dict[word] = vector\n",
    "\n",
    "check_multicollinearity(np.array(list(syl1_pho_dict.values())),\n",
    "                        all_phonemes_syl1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a766d74794da4220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the two and run PCA\n",
    "syl1_aco_pho_dict = {key: syl1_aco_dict[key] + syl1_pho_dict[key]\n",
    "                    for key in syl1_aco_dict}\n",
    "check_multicollinearity(np.array(list(syl1_aco_pho_dict.values())),\n",
    "                        feature_names+all_phonemes_syl1)\n",
    "syl1_aco_pho_dict_pca_df, _, loadings_df = perform_pca_on_acoustic_features(\n",
    "    syl1_aco_pho_dict,original_feature_names=feature_names+all_phonemes_syl1,\n",
    "    n_components=None, # Let the function decide\n",
    "    min_variance_retention=0.9\n",
    ")\n",
    "print(loadings_df)\n",
    "syl1_aco_pho_dict_pca_df=syl1_aco_pho_dict_pca_df.T\n",
    "print(syl1_aco_pho_dict_pca_df)\n",
    "pc_new_names=classify_pca(loadings_df,ratio_threshold=6/4)\n",
    "syl1_aco_pho_dict_pca_df.rename(index=pc_new_names, inplace=True)\n",
    "print(syl1_aco_pho_dict_pca_df)\n",
    "loadings_df=np.abs(loadings_df)\n",
    "sns.heatmap(loadings_df.rename(index=pc_new_names))\n",
    "syl1_aco_pho_dict_pca_df.to_csv(os.path.join(lme_dir,'syl1_aco_pho_dict_pca.csv'), index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250bd80c5e84806c",
   "metadata": {},
   "source": [
    "## 6. Transformer embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527011591b7e2fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "words = list(syllables.keys())\n",
    "print(words)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "word_embeddings = model.encode(words)\n",
    "word_to_embedding = {word: embedding for word, embedding in zip(words, word_embeddings)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb729941892ce53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"valuk：\",word_to_embedding['valuk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94fbfa9dd842e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embd_names=[str(i) for i in range(len(word_to_embedding['valuk']))]\n",
    "check_multicollinearity(np.array(list(word_to_embedding.values())),embd_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee48a76d247bcd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_embedding_pca_df, _, loadings_df = perform_pca_on_acoustic_features(\n",
    "    word_to_embedding,original_feature_names=embd_names,\n",
    "    n_components=None, # Let the function decide\n",
    "    min_variance_retention=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1a78d71e9ffc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(loadings_df)\n",
    "embdpc_names= {f'PC{str(i)}':f'Wordvec{str(i)}' for i in range(word_to_embedding_pca_df.shape[1]+1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58928f9403485df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "random_sample = word_to_embedding_pca_df.iloc[[0, 1, 5, 9, 10, 13, 14, 18, 19, 21]]\n",
    "ax.scatter(random_sample['PC1'], random_sample['PC2'], random_sample['PC3'], c='blue', s=100)\n",
    "\n",
    "for word, row in random_sample.iterrows():\n",
    "    ax.text(row['PC1'], row['PC2'], row['PC3'], word, fontsize=12, zorder=1)\n",
    "\n",
    "ax.set_xlabel('PC1', fontsize=10)\n",
    "ax.set_ylabel('PC2', fontsize=10)\n",
    "ax.set_zlabel('PC3', fontsize=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c307c3f7afbe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word_to_embedding_pca_df=word_to_embedding_pca_df.T\n",
    "print(word_to_embedding_pca_df)\n",
    "word_to_embedding_pca_df.rename(index=embdpc_names,inplace=True)\n",
    "word_to_embedding_pca_df.to_csv(os.path.join(lme_dir,'word_to_embedding_pca.csv'), index=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ieeg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
